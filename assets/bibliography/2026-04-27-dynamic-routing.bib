@inproceedings{huang2024topp,
  title={Harder Task Needs More Experts: Dynamic Routing in MoE Models},
  author={Huang, Quzhe and An, Zhenwei and Zhuang, Nan and Tao, Mingxu and Zhang, Chen and Jin, Yang and Xu, Kun and Chen, Liwei and Huang, Songfang and Feng, Yansong},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={12883--12895},
  year={2024}
}
@inproceedings{guo2025dynmoe,
  title={Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models},
  author={Guo, Yongxin and Cheng, Zhenglin and Tang, Xiaoying and Tu, Zhaopeng and Lin, Tao},
  booktitle={The Thirteenth International Conference on Learning Representations (ICLR)},
  year={2025}
}
@inproceedings{yue2025adak,
  title={Ada-k routing: Boosting the efficiency of moe-based llms},
  author={Yue, Tongtian and Guo, Longteng and Cheng, Jie and Gao, Xuange and Huang, Hua and Liu, Jing},
  booktitle={The Thirteenth International Conference on Learning Representations (ICLR)},
  year={2025}
}
@inproceedings{jin2025moepp,
  title={MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts},
  author={Jin, Peng and Zhu, Bo and Yuan, Li and YAN, Shuicheng},
  booktitle={The Thirteenth International Conference on Learning Representations (ICLR)},
  year={2025}
}
@inproceedings{zeng2024adamoe,
  title={AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models},
  author={Zeng, Zihao and Miao, Yibo and Gao, Hongcheng and Zhang, Hao and Deng, Zhijie},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={6223--6235},
  year={2024}
}
@article{meituan2025longcat,
  title={Longcat-flash technical report},
  author={Meituan LongCat Team},
  journal={arXiv preprint arXiv:2509.01322},
  year={2025}
}
@article{li2025uni-moe-2.0-omni,
  title={Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data},
  author={Li, Yunxin and Chen, Xinyu and Jiang, Shenyuan and Shi, Haoyuan and Liu, Zhenyu and Zhang, Xuanyu and Deng, Nanhao and Xu, Zhenran and Ma, Yicheng and Zhang, Meishan and others},
  journal={arXiv preprint arXiv:2511.12609},
  year={2025}
}
@inproceedings{wang2025remoe,
  title={ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing},
  author={Wang, Ziteng and Zhu, Jun and Chen, Jianfei},
  booktitle={The Thirteenth International Conference on Learning Representations (ICLR)},
  year={2025}
}
@inproceedings{song2025blockffn,
  title={BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity},
  author={Song, Chenyang and Zhao, Weilin and Han, Xu and Xiao, Chaojun and Chen, Yingfa and Li, Yuxuan and Liu, Zhiyuan and Sun, Maosong},
  booktitle={Second Conference on Language Modeling (COLM)},
  year={2025}
}
@article{gale2023megablocks,
  title={Megablocks: Efficient sparse training with mixture-of-experts},
  author={Gale, Trevor and Narayanan, Deepak and Young, Cliff and Zaharia, Matei},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  pages={288--304},
  year={2023}
}
@article{schulman2017ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}