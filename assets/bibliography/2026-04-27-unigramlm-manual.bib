% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
@inproceedings{kudo-2018-subword,
    title = "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates",
    author = "Kudo, Taku",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1007/",
    doi = "10.18653/v1/P18-1007",
    pages = "66--75",
    abstract = "Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings."
}
@inproceedings{schmidt-etal-2024-tokenization,
    title = "Tokenization Is More Than Compression",
    author = "Schmidt, Craig W  and
      Reddy, Varshini  and
      Zhang, Haoran  and
      Alameddine, Alec  and
      Uzan, Omri  and
      Pinter, Yuval  and
      Tanner, Chris",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.40/",
    doi = "10.18653/v1/2024.emnlp-main.40",
    pages = "678--702",
    abstract = "Tokenization is a foundational step in natural language processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document`s text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers. Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction. We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available."
}
@inproceedings{hofmann-etal-2022-embarrassingly,
    title = "An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers",
    author = "Hofmann, Valentin  and
      Schuetze, Hinrich  and
      Pierrehumbert, Janet",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.43/",
    doi = "10.18653/v1/2022.acl-short.43",
    pages = "385--393",
    abstract = "We introduce FLOTA (Few Longest Token Approximation), a simple yet effective method to improve the tokenization of pretrained language models (PLMs). FLOTA uses the vocabulary of a standard tokenizer but tries to preserve the morphological structure of words during tokenization. We evaluate FLOTA on morphological gold segmentations as well as a text classification task, using BERT, GPT-2, and XLNet as example PLMs. FLOTA leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise."
}

@inproceedings{goldman-etal-2024-unpacking,
    title = "Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance",
    author = "Goldman, Omer  and
      Caciularu, Avi  and
      Eyal, Matan  and
      Cao, Kris  and
      Szpektor, Idan  and
      Tsarfaty, Reut",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.134/",
    doi = "10.18653/v1/2024.findings-acl.134",
    pages = "2274--2286",
    abstract = "Despite it being the cornerstone of BPE, the most common tokenization algorithm, the importance of compression in the tokenization process is still unclear. In this paper, we argue for the theoretical importance of compression, that can be viewed as 0-gram language modeling where equal probability is assigned to all tokens. We also demonstrate the empirical importance of compression for downstream success of pre-trained language models. We control the compression ability of several BPE tokenizers by varying the amount of documents available during their training: from 1 million documents to a character-based tokenizer equivalent to no training data at all. We then pre-train English language models based on those tokenizers and fine-tune them over several tasks. We show that there is a correlation between tokenizers' compression and models' downstream performance, suggesting that compression is a reliable intrinsic indicator of tokenization quality. These correlations are more pronounced for generation tasks (over classification) or for smaller models (over large ones). We replicated a representative part of our experiments on Turkish and found similar results, confirming that our results hold for languages with typological characteristics dissimilar to English. We conclude that building better compressing tokenizers is a fruitful avenue for further research and for improving overall model performance."
}
@inproceedings{sennrich-etal-2016-neural,
	title        = {Neural Machine Translation of Rare Words with Subword Units},
	author       = {Sennrich, Rico  and Haddow, Barry  and Birch, Alexandra},
	year         = 2016,
	month        = aug,
	booktitle    = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Berlin, Germany},
	pages        = {1715--1725},
	doi          = {10.18653/v1/P16-1162},
	url          = {https://aclanthology.org/P16-1162},
	editor       = {Erk, Katrin  and Smith, Noah A.}
}
@article{toraman-etal-2023-impact,
	title        = {Impact of Tokenization on Language Models: An Analysis for {T}urkish},
	author       = {Toraman, Cagri and Yilmaz, Eyup Halit and \c{S}ahi\.nu\c{c}, Furkan and Ozcelik, Oguzhan},
	year         = 2023,
	month        = mar,
	journal      = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 22,
	number       = 4,
	doi          = {10.1145/3578707},
	issn         = {2375-4699},
	url          = {https://doi.org/10.1145/3578707},
	issue_date   = {April 2023},
	articleno    = 116,
	numpages     = 21,
	keywords     = {Language model, morphological analysis, tokenization, vocabulary size}
}
@inproceedings{rust-etal-2021-how,
	title        = {How {Good} is {Your} {Tokenizer}? {On} the {Monolingual} {Performance} of {Multilingual} {Language} {Models}},
	shorttitle   = {How {Good} is {Your} {Tokenizer}?},
	author       = {Rust, Phillip and Pfeiffer, Jonas and VuliÄ‡, Ivan and Ruder, Sebastian and Gurevych, Iryna},
	year         = 2021,
	month        = aug,
	booktitle    = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher    = {Association for Computational Linguistics},
	pages        = {3118--3135},
	doi          = {10.18653/v1/2021.acl-long.243},
	url          = {https://aclanthology.org/2021.acl-long.243/},
	urldate      = {2025-04-25},
	editor       = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto}
}
@inproceedings{phan-etal-2025-exact,
    title={Exact Byte-Level Probabilities from Tokenized Language Models for {FIM}-Tasks and Model Ensembles},
    author={Buu Phan and Brandon Amos and Itai Gat and Marton Havasi and Matthew J. Muckley and Karen Ullrich},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=zGej22CBnS}
}
@inproceedings{rajaraman-etal-2024-analysis,
title={An Analysis of Tokenization: {T}ransformers under {Markov} Data},
author={Nived Rajaraman and Jiantao Jiao and Kannan Ramchandran},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=wm9JZq7RCe}
}
@inproceedings{zouhar-etal-2023-tokenization,
    title = "Tokenization and the Noiseless Channel",
    author = "Zouhar, Vil{\'e}m  and
      Meister, Clara  and
      Gastaldi, Juan  and
      Du, Li  and
      Sachan, Mrinmaya  and
      Cotterell, Ryan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.284/",
    doi = "10.18653/v1/2023.acl-long.284",
    pages = "5184--5207",
    abstract = "Subword tokenization is a key part of most NLP pipelines. However, little is known about why some tokenizer and hyperparameter combinations lead to improved downstream model performance over others. We propose that good tokenizers lead to efficient channel usage, where the channel is the means by which some input is conveyed to the model and efficiency can be quantified in information-theoretic terms as the ratio of the Shannon entropy to the maximum entropy of the subword distribution. Nevertheless, an optimal encoding according to Shannon entropy assigns extremely long codes to low-frequency subwords and very short codes to high-frequency subwords.Defining efficiency in terms of R{\'e}nyi entropy, on the other hand, penalizes distributions with either very high or very low-frequency subwords.We posit that (1) extremely high-frequency subwords are problematic because their meaning is not distinct and (2) that low-frequency subwords may not appear frequently enough for their meaning to be learned properly; encodings that induce unigram distributions with either can harm model performance. In machine translation, we find that across multiple tokenizers, the R{\'e}nyi entropy has a very strong correlation with BLEU: 0.82 in comparison to just -0.30 for compressed length."
}
@inproceedings{jiang-etal-2024-peek,
    title = "A Peek into Token Bias: {L}arge Language Models Are Not Yet Genuine Reasoners",
    author = "Jiang, Bowen  and
      Xie, Yangxinyu  and
      Hao, Zhuoqun  and
      Wang, Xiaomeng  and
      Mallick, Tanwi  and
      Su, Weijie J  and
      Taylor, Camillo Jose  and
      Roth, Dan",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.272/",
    doi = "10.18653/v1/2024.emnlp-main.272",
    pages = "4722--4756",
    abstract = "This study introduces a hypothesis-testing framework to assess whether large language models (LLMs) possess genuine reasoning abilities or primarily depend on token bias. We go beyond evaluating LLMs on accuracy; rather, we aim to investigate their token bias in solving logical reasoning tasks. Specifically, we develop carefully controlled synthetic datasets, featuring conjunction fallacy and syllogistic problems. Our framework outlines a list of hypotheses where token biases are readily identifiable, with all null hypotheses assuming genuine reasoning capabilities of LLMs. The findings in this study suggest, with statistical guarantee, that most LLMs still struggle with logical reasoning. While they may perform well on classic problems, their success largely depends on recognizing superficial patterns with strong token bias, thereby raising concerns about their actual reasoning and generalization abilities."
}
@inproceedings{
ahia2024magnet,
title={{MAGNET}: Improving the Multilingual Fairness of Language Models with Adaptive Gradient-Based Tokenization},
author={Orevaoghene Ahia and Sachin Kumar and Hila Gonen and Valentin Hofmann and Tomasz Limisiewicz and Yulia Tsvetkov and Noah A. Smith},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=1e3MOwHSIX}
}
@inproceedings{ahia-etal-2023-languages,
    title = "Do All Languages Cost the Same? {T}okenization in the Era of Commercial Language Models",
    author = "Ahia, Orevaoghene  and
      Kumar, Sachin  and
      Gonen, Hila  and
      Kasai, Jungo  and
      Mortensen, David  and
      Smith, Noah  and
      Tsvetkov, Yulia",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.614/",
    doi = "10.18653/v1/2023.emnlp-main.614",
    pages = "9904--9923",
    abstract = "Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of {\textquotedblleft}tokens{\textquotedblright} processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API`s pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI`s language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable, to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable."
}
@inproceedings{petrov-etal-2023-tokenizers,
author = {Petrov, Aleksandar and Malfa, Emanuele La and Torr, Philip H.S. and Bibi, Adel},
title = {Language model tokenizers introduce unfairness between languages},
year = {2023},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent language models have shown impressive multilingual performance, even when not explicitly trained for it. Despite this, there are concerns about the quality of their outputs across different languages. In this paper, we show how disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked. The same text translated into different languages can have drastically different tokenization lengths, with differences up to 15 times in some cases. These disparities persist even for tokenizers that are intentionally trained for multilingual support. Character-level and byte-level models also exhibit over 4 times the difference in the encoding length for some language pairs. This induces unfair treatment for some language communities in regard to the cost of accessing commercial language services, the processing time and latency, as well as the amount of content that can be provided as context to the models. Therefore, we make the case that we should train future language models using multilingually fair subword tokenizers.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {1608},
numpages = {28},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}
@inproceedings{limisiewicz-etal-2023-tokenization,
    title = "Tokenization Impacts Multilingual Language Modeling: {A}ssessing Vocabulary Allocation and Overlap Across Languages",
    author = "Limisiewicz, Tomasz  and
      Balhar, Ji{\v{r}}{\'i}  and
      Mare{\v{c}}ek, David",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.350/",
    doi = "10.18653/v1/2023.findings-acl.350",
    pages = "5661--5681",
    abstract = "Multilingual language models have recently gained attention as a promising solution for representing multiple languages in a single model. In this paper, we propose new criteria to evaluate the quality of lexical representation and vocabulary overlap observed in sub-word tokenizers.Our findings show that the overlap of vocabulary across languages can be actually detrimental to certain downstream tasks (POS, dependency tree labeling). In contrast, NER and sentence-level tasks (cross-lingual retrieval, NLI) benefit from sharing vocabulary. We also observe that the coverage of the language-specific tokens in the multilingual vocabulary significantly impacts the word-level tasks. Our study offers a deeper understanding of the role of tokenizers in multilingual language models and guidelines for future model developers to choose the most suitable tokenizer for their specific application before undertaking costly model pre-training."
}
@inproceedings{vaswani-etal-2017-attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}
@article{xue-etal-2022-byt5,
    title = "{B}y{T}5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models",
    author = "Xue, Linting  and
      Barua, Aditya  and
      Constant, Noah  and
      Al-Rfou, Rami  and
      Narang, Sharan  and
      Kale, Mihir  and
      Roberts, Adam  and
      Raffel, Colin",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.17/",
    doi = "10.1162/tacl_a_00461",
    pages = "291--306",
    abstract = "Most widely used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: They can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Because byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.1"
}

@inproceedings{downey-etal-2023-embedding,
    title = "Embedding Structure Matters: Comparing Methods to Adapt Multilingual Vocabularies to New Languages",
    author = "Downey, C.m.  and
      Blevins, Terra  and
      Goldfine, Nora  and
      Steinert-Threlkeld, Shane",
    editor = "Ataman, Duygu",
    booktitle = "Proceedings of the 3rd Workshop on Multi-lingual Representation Learning (MRL)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.mrl-1.20/",
    doi = "10.18653/v1/2023.mrl-1.20",
    pages = "268--281"
}

@inproceedings{chizhov-etal-2024-bpe,
    title = "{BPE} Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training",
    author = "Chizhov, Pavel  and
      Arnett, Catherine  and
      Korotkova, Elizaveta  and
      Yamshchikov, Ivan P.",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.925/",
    doi = "10.18653/v1/2024.emnlp-main.925",
    pages = "16587--16604",
    abstract = "Language models can greatly benefit from efficient tokenization. However, they still mostly utilize the classical Byte-Pair Encoding (BPE) algorithm, a simple and reliable method. BPE has been shown to cause such issues as under-trained tokens and sub-optimal compression that may affect the downstream performance. We introduce PickyBPE, a modified BPE algorithm that carries out vocabulary refinement during tokenizer training by removing merges that leave intermediate {\textquotedblleft}junk{\textquotedblright} tokens. Our method improves vocabulary efficiency, eliminates under-trained tokens, and does not compromise text compression. Our experiments show that this method either improves downstream performance or does not harm it."
}

@article{Remy_2024,
  author       = {Fran{\c{c}}ois Remy and
                  Pieter Delobelle and
                  Hayastan Avetisyan and
                  Alfiya Khabibullina and
                  Miryam de Lhoneux and
                  Thomas Demeester},
  title        = {Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language
                  Adaptation of LLMs for Low-Resource {NLP}},
  journal      = {CoRR},
  volume       = {abs/2408.04303},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2408.04303},
  doi          = {10.48550/ARXIV.2408.04303},
  eprinttype    = {arXiv},
  eprint       = {2408.04303},
  timestamp    = {Fri, 13 Sep 2024 20:46:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2408-04303.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{zimmerman24,
author = {Zimmerman, Julia and Hudon, Denis and Cramer, Kathryn and Ruiz, Alejandro and Beauregard, Calla and Fehr, Ashley and Fudolig, Mikaela and Demarest, Bradford and Bird, Yoshi and Trujillo, Milo and Danforth, Christopher and Dodds, Peter},
year = {2024},
month = {12},
pages = {},
title = {Tokens, the oft-overlooked appetizer: Large language models, the distributional hypothesis, and meaning},
doi = {10.48550/arXiv.2412.10924},
journal      = {CoRR},
  volume       = {abs/2412.10924},
  year         = {2024},
  url          = {https://arxiv.org/abs/2412.10924},
}

@inproceedings{cao-rimell-2021-evaluate,
    title = "You should evaluate your language model on marginal likelihood over tokenisations",
    author = "Cao, Kris  and
      Rimell, Laura",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.161/",
    doi = "10.18653/v1/2021.emnlp-main.161",
    pages = "2104--2114",
    abstract = "Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of language model performance. Using only the one-best tokenisation ignores tokeniser uncertainty over alternative tokenisations, which may hurt model out-of-domain performance. In this paper, we argue that instead, language models should be evaluated on their marginal likelihood over tokenisations. We compare different estimators for the marginal likelihood based on sampling, and show that it is feasible to estimate the marginal likelihood with a manageable number of samples. We then evaluate a pretrained language model on both the one-best-tokenisation and marginal perplexities, and show that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data. We link this difference in perplexity to the tokeniser uncertainty as measured by tokeniser entropy. We discuss some implications of our results for language model training and evaluation, particularly with regard to tokenisation robustness."
}