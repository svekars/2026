@inproceedings{sogaard-2017-think,
    title = "What {I} think when {I} think about treebanks",
    author = "S{\o}gaard, Anders",
    editor = "Haji{\v{c}}, Jan",
    booktitle = "Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories",
    year = "2017",
    address = "Prague, Czech Republic",
    url = "https://aclanthology.org/W17-7620/",
    pages = "161--166"
}

@article{ott2022mapping,
  title={Mapping global dynamics of benchmark creation and saturation in artificial intelligence},
  author={Ott, Simon and Barbosa-Silva, Adriano and Blagec, Kathrin and Brauner, Jan and Samwald, Matthias},
  journal={Nature Communications},
  volume={13},
  number={1},
  pages={6793},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{sainz-etal-2023-nlp,
    title = "{NLP} Evaluation in trouble: On the Need to Measure {LLM} Data Contamination for each Benchmark",
    author = "Sainz, Oscar  and
      Campos, Jon  and
      Garc{\'i}a-Ferrero, Iker  and
      Etxaniz, Julen  and
      de Lacalle, Oier Lopez  and
      Agirre, Eneko",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.722/",
    doi = "10.18653/v1/2023.findings-emnlp.722",
    pages = "10776--10787",
    abstract = "In this position paper we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination."
}

@inproceedings{
rane2025position,
title={Position: Principles of Animal Cognition to Improve {LLM} Evaluations},
author={Sunayana Rane and Cyrus F. Kirkman and Graham Todd and Amanda Royka and Ryan M.C. Law and Erica Cartmill and Jacob Gates Foster},
booktitle={Forty-second International Conference on Machine Learning Position Paper Track},
year={2025},
url={https://openreview.net/forum?id=gCPJFcHskT}
}

@inproceedings{
zhuang2025position,
title={Position: {AI} Evaluation Should Learn from How We Test Humans},
author={Yan Zhuang and Qi Liu and Zachary Pardos and Patrick C. Kyllonen and Jiyun Zu and Zhenya Huang and Shijin Wang and Enhong Chen},
booktitle={Forty-second International Conference on Machine Learning Position Paper Track},
year={2025},
url={https://openreview.net/forum?id=MxCJbuJhWG}
}

@article{salaudeen2025measurement,
  title={Measurement to Meaning: A Validity-Centered Framework for AI Evaluation},
  author={Salaudeen, Olawale and Reuel, Anka and Ahmed, Ahmed and Bedi, Suhana and Robertson, Zachary and Sundar, Sudharsan and Domingue, Ben and Wang, Angelina and Koyejo, Sanmi},
  journal={arXiv preprint arXiv:2505.10573},
  year={2025}
}

@inproceedings{bhatt2021case,
  title={A case study of efficacy and challenges in practical human-in-loop evaluation of nlp systems using checklist},
  author={Bhatt, Shaily and Jain, Rahul and Dandapat, Sandipan and Sitaram, Sunayana},
  booktitle={Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval)},
  pages={120--130},
  year={2021}
}

@inproceedings{belz-etal-2023-non,
    title = "Non-Repeatable Experiments and Non-Reproducible Results: The Reproducibility Crisis in Human Evaluation in {NLP}",
    author = "Belz, Anya  and
      Thomson, Craig  and
      Reiter, Ehud  and
      Mille, Simon",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.226/",
    doi = "10.18653/v1/2023.findings-acl.226",
    pages = "3676--3687",
    abstract = "Human evaluation is widely regarded as the litmus test of quality in NLP. A basic requirementof all evaluations, but in particular where they are used for meta-evaluation, is that they should support the same conclusions if repeated. However, the reproducibility of human evaluations is virtually never queried, let alone formally tested, in NLP which means that their repeatability and the reproducibility of their results is currently an open question. This focused contribution reports our review of human evaluation experiments reported in NLP papers over the past five years which we assessed in terms oftheir ability to be rerun. Overall, we estimatethat just 5{\%} of human evaluations are repeatable in the sense that (i) there are no prohibitivebarriers to repetition, and (ii) sufficient information about experimental design is publicly available for rerunning them. Our estimate goesup to about 20{\%} when author help is sought. We complement this investigation with a survey of results concerning the reproducibilityof human evaluations where those are repeatable in the first place. Here we find worryinglylow degrees of reproducibility, both in terms ofsimilarity of scores and of findings supportedby them. We summarise what insights can begleaned so far regarding how to make humanevaluations in NLP more repeatable and morereproducible."
}

@inproceedings{bietti2020ethics,
  title={From ethics washing to ethics bashing: a view on tech ethics from within moral philosophy},
  author={Bietti, Elettra},
  booktitle={Proceedings of the 2020 conference on fairness, accountability, and transparency},
  pages={210--219},
  year={2020}
}

@article{weidinger2025toward,
  title={Toward an evaluation science for generative ai systems},
  author={Weidinger, Laura and Raji, Inioluwa Deborah and Wallach, Hanna and Mitchell, Margaret and Wang, Angelina and Salaudeen, Olawale and Bommasani, Rishi and Ganguli, Deep and Koyejo, Sanmi and Isaac, William},
  journal={arXiv preprint arXiv:2503.05336},
  year={2025}
}

@article{kapoor2023leakage,
  title={Leakage and the reproducibility crisis in machine-learning-based science},
  author={Kapoor, Sayash and Narayanan, Arvind},
  journal={Patterns},
  volume={4},
  number={9},
  year={2023},
  publisher={Elsevier}
}
@inproceedings{belz-etal-2021-systematic,
    title = "A Systematic Review of Reproducibility Research in Natural Language Processing",
    author = "Belz, Anya  and
      Agarwal, Shubham  and
      Shimorina, Anastasia  and
      Reiter, Ehud",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.29/",
    doi = "10.18653/v1/2021.eacl-main.29",
    pages = "381--393",
    abstract = "Against the background of what has been termed a reproducibility crisis in science, the NLP field is becoming increasingly interested in, and conscientious about, the reproducibility of its results. The past few years have seen an impressive range of new initiatives, events and active research in the area. However, the field is far from reaching a consensus about how reproducibility should be defined, measured and addressed, with diversity of views currently increasing rather than converging. With this focused contribution, we aim to provide a wide-angle, and as near as possible complete, snapshot of current work on reproducibility in NLP,"
}
@inproceedings{belz-etal-2025-2025,
    title = "The 2025 {R}epro{NLP} Shared Task on Reproducibility of Evaluations in {NLP}: Overview and Results",
    author = "Belz, Anya  and
      Thomson, Craig  and
      Gonz{\'a}lez Corbelle, Javier  and
      Ruelle, Malo",
    editor = "Arviv, Ofir  and
      Clinciu, Miruna  and
      Dhole, Kaustubh  and
      Dror, Rotem  and
      Gehrmann, Sebastian  and
      Habba, Eliya  and
      Itzhak, Itay  and
      Mille, Simon  and
      Perlitz, Yotam  and
      Santus, Enrico  and
      Sedoc, Jo{\~a}o  and
      Shmueli Scheuer, Michal  and
      Stanovsky, Gabriel  and
      Tafjord, Oyvind",
    booktitle = "Proceedings of the Fourth Workshop on Generation, Evaluation and Metrics (GEM{\texttwosuperior})",
    month = jul,
    year = "2025",
    address = "Vienna, Austria and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.gem-1.78/",
    pages = "1002--1016",
    ISBN = "979-8-89176-261-9",
    abstract = "This paper presents an overview of, and the results from, the 2025 Shared Task on Reproducibility of Evaluations in NLP (ReproNLP{'}25) which followed on from four previous shared tasks on reproducibility of evaluations, ReproNLP{'}24, ReproNLP{'}23, ReproGen{'}22 and ReproGen{'}21. This shared task series forms part of an ongoing research programme designed to develop theory and practice of reproducibility assessment in NLP and machine learning, against a backdrop of increasing recognition of the importance of the topic across the two fields. We describe the ReproNLP{'}25 shared task, summarise results from the reproduction studies submitted, and provide additional comparative analysis of their results, including for the first time additional, `sanity-check' evaluations by LLMs."
}

@article{ihde1961karlsruhe,
  title={The Karlsruhe Congress: a centennial retrospective},
  author={Ihde, Aaron J},
  journal={Journal of Chemical Education},
  volume={38},
  number={2},
  pages={83},
  year={1961},
  publisher={ACS Publications}
}

@article{edwards2021eu,
  title={The EU AI Act: a summary of its significance and scope},
  author={Edwards, Lilian},
  journal={Artificial Intelligence (the EU AI Act)},
  volume={1},
  pages={25},
  year={2021}
}

@article{larsson2020transparency,
  title={Transparency in artificial intelligence},
  author={Larsson, Stefan and Heintz, Fredrik},
  journal={Internet policy review},
  volume={9},
  number={2},
  pages={1--16},
  year={2020},
  publisher={The Alexander Humboldt Institute for Internet and Society}
}

@inproceedings{10.1145/3719384.3719447,
author = {Fan, Xiaojing and Tao, Chunliang},
title = {Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency, Performance, and Adversarial Robustness},
year = {2025},
isbn = {9798400717925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719384.3719447},
doi = {10.1145/3719384.3719447},
abstract = {With the increasing demand for practical applications of Large Language Models (LLMs), many attention-efficient models have been developed to balance performance and computational cost. However, the adversarial robustness of these models remains under-explored. In this work, we design a framework to investigate the trade-off between efficiency, performance, and adversarial robustness of LLMs and conduct extensive experiments on three prominent models with varying levels of complexity and efficiency – Transformer++, Gated Linear Attention (GLA) Transformer, and MatMul-Free LM – utilizing the GLUE and AdvGLUE datasets. The AdvGLUE dataset extends the GLUE dataset with adversarial samples designed to challenge model robustness. Our results show that while the GLA Transformer and MatMul-Free LM achieve slightly lower accuracy on GLUE tasks, they demonstrate higher efficiency and either superior or comparative robustness on AdvGLUE tasks compared to Transformer++ across different attack levels. These findings highlight the potential of simplified architectures to achieve a compelling balance between efficiency, performance, and adversarial robustness, offering valuable insights for applications where resource constraints and resilience to adversarial attacks are critical.},
booktitle = {Proceedings of the 2024 7th Artificial Intelligence and Cloud Computing Conference},
pages = {429–436},
numpages = {8},
keywords = {Adversarial Attacks, Computational Efficiency, Large Language Models, Robustness},
location = {
},
series = {AICCC '24}
}

@article{ai2023artificial,
  title={Artificial intelligence risk management framework (AI RMF 1.0)},
  author={AI, NIST},
  journal={URL: https://nvlpubs. nist. gov/nistpubs/ai/nist. ai},
  pages={100--1},
  year={2023}
}

@book{goldacre2009bad,
  title={Bad science},
  author={Goldacre, Ben},
  year={2009},
  publisher={Harper Perennial}
}

@article{lanamaki2025expect,
  title={What to Expect from the Upcoming EU AI Act Sandboxes: Panel Report},
  author={Lanam{\"a}ki, Arto and V{\"a}yrynen, Karin and Vainionp{\"a}{\"a}, Fanny and Hietala, Heidi and Tervo, Erkki and Moltzau, Alex and Weerts, Sophie and Niemeyer, Demian},
  journal={Digital Society},
  volume={4},
  number={2},
  pages={42},
  year={2025},
  publisher={Springer}
}

@article{sokol2024benchmarkcards,
  title={BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks},
  author={Sokol, Anna and Daly, Elizabeth and Hind, Michael and Piorkowski, David and Zhang, Xiangliang and Moniz, Nuno and Chawla, Nitesh},
  journal={arXiv preprint arXiv:2410.12974},
  year={2024}
}

@inproceedings{samsi2023words,
  title={From words to watts: Benchmarking the energy costs of large language model inference},
  author={Samsi, Siddharth and Zhao, Dan and McDonald, Joseph and Li, Baolin and Michaleas, Adam and Jones, Michael and Bergeron, William and Kepner, Jeremy and Tiwari, Devesh and Gadepally, Vijay},
  booktitle={2023 IEEE High Performance Extreme Computing Conference (HPEC)},
  pages={1--9},
  year={2023},
  organization={IEEE}
}

@article{husom2025sustainable,
  title={Sustainable llm inference for edge ai: Evaluating quantized llms for energy efficiency, output accuracy, and inference latency},
  author={Husom, Erik Johannes and Goknil, Arda and Astekin, Merve and Shar, Lwin Khin and K{\aa}sen, Andre and Sen, Sagar and Mithassel, Benedikt Andreas and Soylu, Ahmet},
  journal={ACM Transactions on Internet of Things},
  year={2025},
  publisher={ACM New York, NY}
}

@incollection{agrawal2024accountability,
  title={Accountability, trust, and transparency in AI systems from the perspective of public policy: Elevating ethical standards},
  author={Agrawal, Gaurav},
  booktitle={AI healthcare applications and security, ethical, and legal considerations},
  pages={148--162},
  year={2024},
  publisher={IGI Global}
}

@article{nagendran2020artificial,
  title={Artificial intelligence versus clinicians: systematic review of design, reporting standards, and claims of deep learning studies},
  author={Nagendran, Myura and Chen, Yang and Lovejoy, Christopher A and Gordon, Anthony C and Komorowski, Matthieu and Harvey, Hugh and Topol, Eric J and Ioannidis, John PA and Collins, Gary S and Maruthappu, Mahiben},
  journal={bmj},
  volume={368},
  year={2020},
  publisher={British Medical Journal Publishing Group}
}

@article{laux2024three,
  title={Three pathways for standardisation and ethical disclosure by default under the European Union Artificial Intelligence Act},
  author={Laux, Johann and Wachter, Sandra and Mittelstadt, Brent},
  journal={Computer Law \& Security Review},
  volume={53},
  pages={105957},
  year={2024},
  publisher={Elsevier}
}

@article{pande2023navigating,
  title={Navigating the governance challenges of disruptive technologies: insights from regulation of autonomous systems in Singapore},
  author={Pande, Devyani and Taeihagh, Araz},
  journal={Journal of Economic Policy Reform},
  volume={26},
  number={3},
  pages={298--319},
  year={2023},
  publisher={Taylor \& Francis}
}

@article{dong2024meta,
  title={Meta-Regulation: An ideal alternative to the primary responsibility as the regulatory model of generative AI in China},
  author={Dong, Huijuan and Chen, Junkai},
  journal={Computer Law \& Security Review},
  volume={54},
  pages={106016},
  year={2024},
  publisher={Elsevier}
}

@article{carey2025regulating,
  title={Regulating Uncertainty: Governing General-Purpose AI Models and Systemic Risk},
  author={Carey, Samuel},
  journal={European Journal of Risk Regulation},
  pages={1--17},
  year={2025},
  publisher={Cambridge University Press}
}

@article{sloane2025systematic,
  title={A systematic review of regulatory strategies and transparency mandates in AI regulation in Europe, the United States, and Canada},
  author={Sloane, Mona and W{\"u}llhorst, Elena},
  journal={Data \& Policy},
  volume={7},
  pages={e11},
  year={2025},
  publisher={Cambridge University Press}
}

@article{luo2025lack,
  title={Lack of Methodological Rigor and Limited Coverage of Generative AI in Existing AI Reporting Guidelines: A Scoping Review},
  author={Luo, Xufei and Wang, Bingyi and Shi, Qianling and Wang, Zijun and Lai, Honghao and Liu, Hui and Qin, Yishan and Chen, Fengxian and Song, Xuping and Ge, Long and others},
  journal={Journal of Clinical Epidemiology},
  pages={111903},
  year={2025},
  publisher={Elsevier}
}

@inproceedings{10.1145/3706598.3713240,
author = {Ben chaaben, Eya and Koch, Janin and Mackay, Wendy E.},
title = {"Should I choose a smaller model?': Understanding ML Model Selection and Its Impact on Sustainability},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713240},
doi = {10.1145/3706598.3713240},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1008},
numpages = {13},
location = {
},
series = {CHI '25}
}

@inproceedings{belz-thomson-2024-2024,
    title = "The 2024 {R}epro{NLP} Shared Task on Reproducibility of Evaluations in {NLP}: Overview and Results",
    author = "Belz, Anya  and
      Thomson, Craig",
    editor = "Balloccu, Simone  and
      Belz, Anya  and
      Huidrom, Rudali  and
      Reiter, Ehud  and
      Sedoc, Joao  and
      Thomson, Craig",
    booktitle = "Proceedings of the Fourth Workshop on Human Evaluation of NLP Systems (HumEval) @ LREC-COLING 2024",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.humeval-1.9/",
    pages = "91--105",
    abstract = "This paper presents an overview of, and the results from, the 2024 Shared Task on Reproducibility of Evaluations in NLP (ReproNLP{'}24), following on from three previous shared tasks on reproducibility of evaluations in NLP, ReproNLP{'}23, ReproGen{'}22 and ReproGen{'}21. This shared task series forms part of an ongoing research programme designed to develop theory and practice of reproducibility assessment in NLP and machine learning, against a backdrop of increasing recognition of the importance of reproducibility across the two fields. We describe the ReproNLP{'}24 shared task, summarise results from the reproduction studies submitted, and provide additional comparative analysis of their results."
}

@article{semmelrock2025reproducibility,
  title={Reproducibility in machine-learning-based research: Overview, barriers, and drivers},
  author={Semmelrock, Harald and Ross-Hellauer, Tony and Kopeinik, Simone and Theiler, Dieter and Haberl, Armin and Thalmann, Stefan and Kowald, Dominik},
  journal={AI Magazine},
  volume={46},
  number={2},
  pages={e70002},
  year={2025},
  publisher={Wiley Online Library}
}

@inproceedings{bouthillier2019unreproducible,
  title={Unreproducible research is reproducible},
  author={Bouthillier, Xavier and Laurent, C{\'e}sar and Vincent, Pascal},
  booktitle={International Conference on Machine Learning},
  pages={725--734},
  year={2019},
  organization={PMLR}
}

@inproceedings{dodge-etal-2019-show,
    title = "Show Your Work: Improved Reporting of Experimental Results",
    author = "Dodge, Jesse  and
      Gururangan, Suchin  and
      Card, Dallas  and
      Schwartz, Roy  and
      Smith, Noah A.",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1224/",
    doi = "10.18653/v1/D19-1224",
    pages = "2185--2194",
    abstract = "Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique."
}

@inproceedings{zhao-etal-2025-sphere,
    title = "{SPHERE}: An Evaluation Card for Human-{AI} Systems",
    author = "Zhao, Dora  and
      Ma, Qianou  and
      Zhao, Xinran  and
      Si, Chenglei  and
      Yang, Chenyang  and
      Louie, Ryan  and
      Reiter, Ehud  and
      Yang, Diyi  and
      Wu, Tongshuang",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.70/",
    doi = "10.18653/v1/2025.findings-acl.70",
    pages = "1340--1365",
    ISBN = "979-8-89176-256-5",
    abstract = "In the era of Large Language Models (LLMs), establishing effective evaluation methods and standards for diverse human-AI interaction systems is increasingly challenging. To encourage more transparent documentation and facilitate discussion on human-AI system evaluation design options, we present an evaluation card SPHERE, which encompasses five key dimensions: 1) What is being evaluated?; 2) How is the evaluation conducted?; 3) Who is participating in the evaluation?; 4) When is evaluation conducted?; 5) How is evaluation validated? We conduct a review of 39 human-AI systems using SPHERE, outlining current evaluation practices and areas for improvement. We provide three recommendations for improving the validity and rigor of evaluation practices."
}

@inproceedings{belz-etal-2025-standard,
    title = "Standard Quality Criteria Derived from Current {NLP} Evaluations for Guiding Evaluation Design and Grounding Comparability and {AI} Compliance Assessments",
    author = "Belz, Anya  and
      Mille, Simon  and
      Thomson, Craig",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.1370/",
    doi = "10.18653/v1/2025.findings-acl.1370",
    pages = "26685--26715",
    ISBN = "979-8-89176-256-5",
    abstract = "Research shows that two evaluation experiments reporting results for the same quality criterion name (e.g. Fluency) do not necessarily evaluate the same aspect of quality. Not knowing when two evaluations are comparable in this sense means we currently lack the ability to draw conclusions based on multiple independently conducted evaluations. It is hard to see how this issue can be fully addressed other than by the creation of a standard set of quality criterion names and definitions that the evaluations in use in NLP can be grounded in. Taking a descriptivist approach, the QCET Quality Criteria for Evaluation Taxonomy derives a standard set of 114 quality criterion names and definitions from three surveys of a combined total of 933 evaluation experiments in NLP, and structures them into a reference taxonomy. We present QCET and its uses in (i) establishing comparability of existing evaluations, (ii) guiding the design of new evaluations, and (iii) assessing regulation compliance."
}


@inproceedings{sogaard-etal-2021-need,
    title = "We Need To Talk About Random Splits",
    author = "S{\o}gaard, Anders  and
      Ebert, Sebastian  and
      Bastings, Jasmijn  and
      Filippova, Katja",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.156/",
    doi = "10.18653/v1/2021.eacl-main.156",
    pages = "1823--1832",
    abstract = "(CITATION) argued for using random splits rather than standard splits in NLP experiments. We argue that random splits, like standard splits, lead to overly optimistic performance estimates. We can also split data in biased or adversarial ways, e.g., training on short sentences and evaluating on long ones. Biased sampling has been used in domain adaptation to simulate real-world drift; this is known as the covariate shift assumption. In NLP, however, even worst-case splits, maximizing bias, often under-estimate the error observed on new samples of in-domain data, i.e., the data that models should minimally generalize to at test time. This invalidates the covariate shift assumption. Instead of using multiple random splits, future benchmarks should ideally include multiple, independent test sets instead; if infeasible, we argue that multiple biased splits leads to more realistic performance estimates than multiple random splits."
}

@article{singh2025leaderboard,
  title={The leaderboard illusion},
  author={Singh, Shivalika and Nan, Yiyang and Wang, Alex and D'Souza, Daniel and Kapoor, Sayash and {\"U}st{\"u}n, Ahmet and Koyejo, Sanmi and Deng, Yuntian and Longpre, Shayne and Smith, Noah A and others},
  journal={arXiv preprint arXiv:2504.20879},
  year={2025}
}

@inproceedings{yu-etal-2022-measuring,
    title = "Measuring Robustness for {NLP}",
    author = "Yu, Yu  and
      Khan, Abdul Rafae  and
      Xu, Jia",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.343/",
    pages = "3908--3916",
    abstract = "The quality of Natural Language Processing (NLP) models is typically measured by the accuracy or error rate of a predefined test set. Because the evaluation and optimization of these measures are narrowed down to a specific domain like news and cannot be generalized to other domains like Twitter, we often observe that a system reported with human parity results generates surprising errors in real-life use scenarios. We address this weakness with a new approach that uses an NLP quality measure based on robustness. Unlike previous work that has defined robustness using Minimax to bound worst cases, we measure robustness based on the consistency of cross-domain accuracy and introduce the coefficient of variation and (epsilon, gamma)-Robustness. Our measures demonstrate higher agreements with human evaluation than accuracy scores like BLEU on ranking Machine Translation (MT) systems. Our experiments of sentiment analysis and MT tasks show that incorporating our robustness measures into learning objectives significantly enhances the final NLP prediction accuracy over various domains, such as biomedical and social media."
}



@inproceedings{naik-etal-2018-stress,
    title = "Stress Test Evaluation for Natural Language Inference",
    author = "Naik, Aakanksha  and
      Ravichander, Abhilasha  and
      Sadeh, Norman  and
      Rose, Carolyn  and
      Neubig, Graham",
    editor = "Bender, Emily M.  and
      Derczynski, Leon  and
      Isabelle, Pierre",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1198/",
    pages = "2340--2353",
    abstract = "Natural language inference (NLI) is the task of determining if a natural language hypothesis can be inferred from a given premise in a justifiable manner. NLI was proposed as a benchmark task for natural language understanding. Existing models perform well at standard datasets for NLI, achieving impressive results across different genres of text. However, the extent to which these models understand the semantic content of sentences is unclear. In this work, we propose an evaluation methodology consisting of automatically constructed ``stress tests'' that allow us to examine whether systems have the ability to make real inferential decisions. Our evaluation of six sentence-encoder models on these stress tests reveals strengths and weaknesses of these models with respect to challenging linguistic phenomena, and suggests important directions for future work in this area."
}








@inproceedings{jones1994towards,
  title={Towards better NLP system evaluation},
  author={Jones, Karen Sp{\"a}rck},
  booktitle={Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994},
  year={1994}
}

@article{church2011pendulum,
  title={A pendulum swung too far},
  author={Church, Kenneth},
  journal={Linguistic Issues in Language Technology},
  volume={6},
  year={2011}
}

@article{church2017emerging,
  title={Emerging trends: I did it, I did it, I did it, but...},
  author={Church, Kenneth Ward},
  journal={Natural Language Engineering},
  volume={23},
  number={3},
  pages={473--480},
  year={2017},
  publisher={Cambridge University Press}
}

@article{church2019survey,
  title={A survey of 25 years of evaluation},
  author={Church, Kenneth Ward and Hestness, Joel},
  journal={Natural Language Engineering},
  volume={25},
  number={6},
  pages={753--767},
  year={2019},
  publisher={Cambridge University Press}
}

@inproceedings{linzen-2020-accelerate,
    title = "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
    author = "Linzen, Tal",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.465/",
    doi = "10.18653/v1/2020.acl-main.465",
    pages = "5210--5217",
    abstract = "This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans."
}

@inproceedings{bowman-dahl-2021-will,
    title = "What Will it Take to Fix Benchmarking in Natural Language Understanding?",
    author = "Bowman, Samuel R.  and
      Dahl, George",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.385/",
    doi = "10.18653/v1/2021.naacl-main.385",
    pages = "4843--4855",
    abstract = "Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this position paper, we lay out four criteria that we argue NLU benchmarks should meet. We argue most current benchmarks fail at these criteria, and that adversarial data collection does not meaningfully address the causes of these failures. Instead, restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets, the reliability with which they are annotated, their size, and the ways they handle social bias."
}

@inproceedings{kiela-etal-2021-dynabench,
    title = "Dynabench: Rethinking Benchmarking in {NLP}",
    author = "Kiela, Douwe  and
      Bartolo, Max  and
      Nie, Yixin  and
      Kaushik, Divyansh  and
      Geiger, Atticus  and
      Wu, Zhengxuan  and
      Vidgen, Bertie  and
      Prasad, Grusha  and
      Singh, Amanpreet  and
      Ringshia, Pratik  and
      Ma, Zhiyi  and
      Thrush, Tristan  and
      Riedel, Sebastian  and
      Waseem, Zeerak  and
      Stenetorp, Pontus  and
      Jia, Robin  and
      Bansal, Mohit  and
      Potts, Christopher  and
      Williams, Adina",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.324/",
    doi = "10.18653/v1/2021.naacl-main.324",
    pages = "4110--4124",
    abstract = "We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field."
}




@book{weiner2003piltdown,
  title={The Piltdown Forgery: with a new introduction and afterword by Chris Stringer},
  author={Weiner, Joseph Sidney},
  year={2003},
  publisher={Oxford University Press}
}

@article{vincent1999piltdown,
  title={Piltdown man: Combining the instruction of scientific ethics and qualitative analysis},
  author={Vincent, John B},
  journal={Journal of chemical education},
  volume={76},
  number={11},
  pages={1501},
  year={1999},
  publisher={ACS Publications}
}

@incollection{langdon2016case,
  title={Case study 4. Self-correcting science: the Piltdown forgery},
  author={Langdon, John H},
  booktitle={The science of human evolution: getting it right},
  pages={25--35},
  year={2016},
  publisher={Springer}
}

@article{nissim2017sharing,
  title={Sharing is caring: The future of shared tasks},
  author={Nissim, Malvina and Abzianidze, Lasha and Evang, Kilian and Van Der Goot, Rob and Haagsma, Hessel and Plank, Barbara and Wieling, Martijn},
  journal={Computational Linguistics},
  volume={43},
  number={4},
  pages={897--904},
  year={2017},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{martin2022nlpsharedtasks,
  title={NLPSharedTasks: A Corpus of Shared Task Overview Papers in Natural Language Processing Domains},
  author={Martin, Anna and Pedersen, Ted and D’Souza, Jennifer},
  booktitle={Proceedings of the first Workshop on Information Extraction from Scientific Publications},
  pages={105--120},
  year={2022}
}

@inproceedings{pramanick-etal-2025-nature,
    title = "The Nature of {NLP}: Analyzing Contributions in {NLP} Papers",
    author = "Pramanick, Aniket  and
      Hou, Yufang  and
      Mohammad, Saif M.  and
      Gurevych, Iryna",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.1224/",
    doi = "10.18653/v1/2025.acl-long.1224",
    pages = "25169--25191",
    ISBN = "979-8-89176-251-0",
    abstract = "Natural Language Processing (NLP) is an established and dynamic field. Despite this, what constitutes NLP research remains debated. In this work, we address the question by quantitatively examining NLP research papers. We propose a taxonomy of research contributions and introduce {\_}NLPContributions{\_}, a dataset of nearly $2k$ NLP research paper abstracts, carefully annotated to identify scientific contributions and classify their types according to this taxonomy. We also introduce a novel task of automatically identifying contribution statements and classifying their types from research papers. We present experimental results for this task and apply our model to {\textasciitilde}$29k$ NLP research papers to analyze their contributions, aiding in the understanding of the nature of NLP research. We show that NLP research has taken a winding path {---} with the focus on language and human-centric studies being prominent in the 1970s and 80s, tapering off in the 1990s and 2000s, and starting to rise again since the late 2010s. Alongside this revival, we observe a steady rise in dataset and methodological contributions since the 1990s, such that today, on average, individual NLP papers contribute in more ways than ever before. Our dataset and analyses offer a powerful lens for tracing research trends and offer potential for generating informed, data-driven literature surveys."
}

@inproceedings{gillis-webber-tittel-2020-framework,
    title = "A Framework for Shared Agreement of Language Tags beyond {ISO} 639",
    author = "Gillis-Webber, Frances  and
      Tittel, Sabine",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.408/",
    pages = "3333--3339",
    language = "eng",
    ISBN = "979-10-95546-34-4",
    abstract = "The identification and annotation of languages in an unambiguous and standardized way is essential for the description of linguistic data. It is the prerequisite for machine-based interpretation, aggregation, and re-use of the data with respect to different languages. This makes it a key aspect especially for Linked Data and the multilingual Semantic Web. The standard for language tags is defined by IETF`s BCP 47 and ISO 639 provides the language codes that are the tags' main constituents. However, for the identification of lesser-known languages, endangered languages, regional varieties or historical stages of a language, the ISO 639 codes are insufficient. Also, the optional language sub-tags compliant with BCP 47 do not offer a possibility fine-grained enough to represent linguistic variation. We propose a versatile pattern that extends the BCP 47 sub-tag {\textquoteleft}privateuse' and is, thus, able to overcome the limits of BCP 47 and ISO 639. Sufficient coverage of the pattern is demonstrated with the use case of linguistic Linked Data of the endangered Gascon language. We show how to use a URI shortcode for the extended sub-tag, making the length compliant with BCP 47. We achieve this with a web application and API developed to encode and decode the language tag."
}
@inproceedings{dalby-etal-2004-standards,
    title = "Standards for Language Codes: developing {ISO} 639",
    author = "Dalby, David  and
      Gillam, Lee  and
      Cox, Christopher  and
      Garside, Debbie",
    editor = "Lino, Maria Teresa  and
      Xavier, Maria Francisca  and
      Ferreira, F{\'a}tima  and
      Costa, Rute  and
      Silva, Raquel",
    booktitle = "Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}`04)",
    month = may,
    year = "2004",
    address = "Lisbon, Portugal",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L04-1178/"
}

@inproceedings{hershcovich-etal-2022-towards,
    title = "Towards Climate Awareness in {NLP} Research",
    author = "Hershcovich, Daniel  and
      Webersinke, Nicolas  and
      Kraus, Mathias  and
      Bingler, Julia  and
      Leippold, Markus",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.159/",
    doi = "10.18653/v1/2022.emnlp-main.159",
    pages = "2480--2494",
    abstract = "The climate impact of AI, and NLP research in particular, has become a serious issue given the enormous amount of energy that is increasingly being used for training and running computational models. Consequently, increasing focus is placed on efficient NLP. However, this important initiative lacks simple guidelines that would allow for systematic climate reporting of NLP research. We argue that this deficiency is one of the reasons why very few publications in NLP report key figures that would allow a more thorough examination of environmental impact, and present a quantitative survey to demonstrate this. As a remedy, we propose a climate performance model card with the primary purpose of being practically usable with only limited information about experiments and the underlying computer hardware. We describe why this step is essential to increase awareness about the environmental impact of NLP research and, thereby, paving the way for more thorough discussions."
}



@inproceedings{sun2020we,
  title={Are we evaluating rigorously? benchmarking recommendation for reproducible evaluation and fair comparison},
  author={Sun, Zhu and Yu, Di and Fang, Hui and Yang, Jie and Qu, Xinghua and Zhang, Jie and Geng, Cong},
  booktitle={Proceedings of the 14th ACM Conference on Recommender Systems},
  pages={23--32},
  year={2020}
}

@inproceedings{hershcovich-etal-2022-challenges,
    title = "Challenges and Strategies in Cross-Cultural {NLP}",
    author = "Hershcovich, Daniel  and
      Frank, Stella  and
      Lent, Heather  and
      de Lhoneux, Miryam  and
      Abdou, Mostafa  and
      Brandl, Stephanie  and
      Bugliarello, Emanuele  and
      Cabello Piqueras, Laura  and
      Chalkidis, Ilias  and
      Cui, Ruixiang  and
      Fierro, Constanza  and
      Margatina, Katerina  and
      Rust, Phillip  and
      S{\o}gaard, Anders",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.482/",
    doi = "10.18653/v1/2022.acl-long.482",
    pages = "6997--7013",
    abstract = "Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies."
}

@inproceedings{laskar2024systematic,
  title={A systematic survey and critical review on evaluating large language models: Challenges, limitations, and recommendations},
  author={Laskar, Md Tahmid Rahman and Alqahtani, Sawsan and Bari, M Saiful and Rahman, Mizanur and Khan, Mohammad Abdullah Matin and Khan, Haidar and Jahan, Israt and Bhuiyan, Amran and Tan, Chee Wei and Parvez, Md Rizwan and others},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={13785--13816},
  year={2024}
}

@inproceedings{azime-etal-2025-proverbeval,
    title = "{P}roverb{E}val: Exploring {LLM} Evaluation Challenges for Low-resource Language Understanding",
    author = "Azime, Israel Abebe  and
      Tonja, Atnafu Lambebo  and
      Belay, Tadesse Destaw  and
      Chanie, Yonas  and
      Balcha, Bontu Fufa  and
      Abadi, Negasi Haile  and
      Ademtew, Henok Biadglign  and
      Nerea, Mulubrhan Abebe  and
      Yadeta, Debela Desalegn  and
      Geremew, Derartu Dagne  and
      Tesfu, Assefa Atsbiha  and
      Slusallek, Philipp  and
      Solorio, Thamar  and
      Klakow, Dietrich",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2025",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-naacl.350/",
    pages = "6250--6266",
    ISBN = "979-8-89176-195-7"
}

@article{dietz2025llm,
  title={LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations},
  author={Dietz, Laura and Zendel, Oleg and Bailey, Peter and Clarke, Charles and Cotterill, Ellese and Dalton, Jeff and Hasibi, Faegheh and Sanderson, Mark and Craswell, Nick},
  journal={arXiv preprint arXiv:2504.19076},
  year={2025}
}

@inproceedings{zhuo-etal-2024-prosa,
    title = "{P}ro{SA}: Assessing and Understanding the Prompt Sensitivity of {LLM}s",
    author = "Zhuo, Jingming  and
      Zhang, Songyang  and
      Fang, Xinyu  and
      Duan, Haodong  and
      Lin, Dahua  and
      Chen, Kai",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.108/",
    doi = "10.18653/v1/2024.findings-emnlp.108",
    pages = "1950--1976",
    abstract = "Large language models (LLMs) have demonstrated impressive capabilities across various tasks, but their performance is highly sensitive to the prompts utilized. This variability poses challenges for accurate assessment and user satisfaction. Current research frequently overlooks instance-level prompt variations and their implications on subjective evaluations. To address these shortcomings, we introduce \textbf{ProSA}, a framework designed to evaluate and comprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity metric, PromptSensiScore, and leverages decoding confidence to elucidate underlying mechanisms. Our extensive study, spanning multiple tasks, uncovers that prompt sensitivity fluctuates across datasets and models, with larger models exhibiting enhanced robustness. We observe that few-shot examples can alleviate this sensitivity issue, and subjective evaluations are also susceptible to prompt sensitivities, particularly in complex, reasoning-oriented tasks. Furthermore, our findings indicate that higher model confidence correlates with increased prompt robustness. We believe this work will serve as a helpful tool in studying prompt sensitivity of LLMs. The project is released at: \url{https://github.com/open-compass/ProSA}."
}

@inproceedings{chatterjee-etal-2024-posix,
    title = "{POSIX}: A Prompt Sensitivity Index For Large Language Models",
    author = "Chatterjee, Anwoy  and
      Renduchintala, H S V N S Kowndinya  and
      Bhatia, Sumit  and
      Chakraborty, Tanmoy",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.852/",
    doi = "10.18653/v1/2024.findings-emnlp.852",
    pages = "14550--14565",
    abstract = "Despite their remarkable capabilities, Large Language Models (LLMs) are found to be surprisingly sensitive to minor variations in prompts, often generating significantly divergent outputs in response to minor variations in the prompts, such as spelling errors, alteration of wording or the prompt template. However, while assessing the quality of an LLM, the focus often tends to be solely on its performance on downstream tasks, while very little to no attention is paid to prompt sensitivity. To fill this gap, we propose POSIX {--} a novel PrOmpt Sensitivity IndeX as a reliable measure of prompt sensitivity, thereby offering a more comprehensive evaluation of LLM performance. The key idea behind POSIX is to capture the relative change in loglikelihood of a given response upon replacing the corresponding prompt with a different intent-preserving prompt. We provide thorough empirical evidence demonstrating the efficacy of POSIX in capturing prompt sensitivity and subsequently use it to measure and thereby compare prompt sensitivity of various open source LLMs. We find that merely increasing the parameter count or instruction tuning does not necessarily reduce prompt sensitivity whereas adding some few-shot exemplars, even just one, almost always leads to significant decrease in prompt sensitivity. We also find that alterations to prompt template lead to the highest sensitivity in the case of MCQ type tasks, whereas paraphrasing results in the highest sensitivity in open-ended generation tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinya-renduchintala/POSIX."
}

@inproceedings{10.1145/3544548.3581503,
author = {Jo, Eunkyung and Epstein, Daniel A. and Jung, Hyunhoon and Kim, Young-Ho},
title = {Understanding the Benefits and Challenges of Deploying Conversational AI Leveraging Large Language Models for Public Health Intervention},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581503},
doi = {10.1145/3544548.3581503},
abstract = {Recent large language models (LLMs) have advanced the quality of open-ended conversations with chatbots. Although LLM-driven chatbots have the potential to support public health interventions by monitoring populations at scale through empathetic interactions, their use in real-world settings is underexplored. We thus examine the case of CareCall, an open-domain chatbot that aims to support socially isolated individuals via check-up phone calls and monitoring by teleoperators. Through focus group observations and interviews with 34 people from three stakeholder groups, including the users, the teleoperators, and the developers, we found CareCall offered a holistic understanding of each individual while offloading the public health workload and helped mitigate loneliness and emotional burdens. However, our findings highlight that traits of LLM-driven chatbots led to challenges in supporting public and personal health needs. We discuss considerations of designing and deploying LLM-driven chatbots for public health intervention, including tensions among stakeholders around system expectations.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {18},
numpages = {16},
keywords = {Chatbot, Check-up calls, Large language model, Open-domain dialog system, Public health, Social isolation},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{choi2024unlock,
  title={Unlock life with a Chat (GPT): Integrating conversational AI with large language models into everyday lives of autistic individuals},
  author={Choi, Dasom and Lee, Sunok and Kim, Sung-In and Lee, Kyungah and Yoo, Hee Jeong and Lee, Sangsu and Hong, Hwajung},
  booktitle={Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  pages={1--17},
  year={2024}
}

@inproceedings{wen2024ai,
  title={AI for education (AI4EDU): Advancing personalized education with LLM and adaptive learning},
  author={Wen, Qingsong and Liang, Jing and Sierra, Carles and Luckin, Rose and Tong, Richard and Liu, Zitao and Cui, Peng and Tang, Jiliang},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={6743--6744},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{wang2022omnivl,
  title={Omnivl: One foundation model for image-language and video-language tasks},
  author={Wang, Junke and Chen, Dongdong and Wu, Zuxuan and Luo, Chong and Zhou, Luowei and Zhao, Yucheng and Xie, Yujia and Liu, Ce and Jiang, Yu-Gang and Yuan, Lu},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={5696--5710},
  year={2022}
}

@article{wang2023visionllm,
  title={Visionllm: Large language model is also an open-ended decoder for vision-centric tasks},
  author={Wang, Wenhai and Chen, Zhe and Chen, Xiaokang and Wu, Jiannan and Zhu, Xizhou and Zeng, Gang and Luo, Ping and Lu, Tong and Zhou, Jie and Qiao, Yu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={61501--61513},
  year={2023}
}

@article{wang2024cogvlm,
  title={Cogvlm: Visual expert for pretrained language models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and XiXuan, Song and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={121475--121499},
  year={2024}
}

@article{lee2024vhelm,
  title={Vhelm: A holistic evaluation of vision language models},
  author={Lee, Tony and Tu, Haoqin and Wong, Chi Heem and Zheng, Wenhao and Zhou, Yiyang and Mai, Yifan and Roberts, Josselin and Yasunaga, Michihiro and Yao, Huaxiu and Xie, Cihang and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={140632--140666},
  year={2024}
}

@article{wu2024visionllm,
  title={Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks},
  author={Wu, Jiannan and Zhong, Muyan and Xing, Sen and Lai, Zeqiang and Liu, Zhaoyang and Chen, Zhe and Wang, Wenhai and Zhu, Xizhou and Lu, Lewei and Lu, Tong and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={69925--69975},
  year={2024}
}

@article{duan2019artificial,
  title={Artificial intelligence for decision making in the era of Big Data--evolution, challenges and research agenda},
  author={Duan, Yanqing and Edwards, John S and Dwivedi, Yogesh K},
  journal={International journal of information management},
  volume={48},
  pages={63--71},
  year={2019},
  publisher={Elsevier}
}

@article{araujo2020ai,
  title={In AI we trust? Perceptions about automated decision-making by artificial intelligence},
  author={Araujo, Theo and Helberger, Natali and Kruikemeier, Sanne and De Vreese, Claes H},
  journal={AI \& society},
  volume={35},
  number={3},
  pages={611--623},
  year={2020},
  publisher={Springer}
}

@inproceedings{gruetzemacher2023international,
  title={An International Consortium for AI Risk Evaluations},
  author={Gruetzemacher, Ross and Chan, Alan and Los, {\v{S}}t{\v{e}}p{\'a}n and Frazier, Kevin and Campos, Sim{\'e}on and Franklin, Matija and Fox, James and Hernandez-Orallo, Jose and Manning, Christin and Tomei, Philip and others},
  booktitle={Socially Responsible Language Modelling Research},
  year={2023}
}

@article{hubinger2025anthropic,
  title={Anthropic: Responsible Scaling Policy},
  author={Hubinger, Evan},
  journal={SuperIntelligence-Robotics-Safety \& Alignment},
  volume={2},
  number={1},
  year={2025}
}

@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM transactions on intelligent systems and technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}

@article{gao2025llm,
  title={Llm-based nlg evaluation: Current status and challenges},
  author={Gao, Mingqi and Hu, Xinyu and Yin, Xunjian and Ruan, Jie and Pu, Xiao and Wan, Xiaojun},
  journal={Computational Linguistics},
  pages={1--28},
  year={2025},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}


@article{mizrahi2024state,
  title={State of what art? a call for multi-prompt llm evaluation},
  author={Mizrahi, Moran and Kaplan, Guy and Malkin, Dan and Dror, Rotem and Shahaf, Dafna and Stanovsky, Gabriel},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={933--949},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{hu2024unveiling,
  title={Unveiling llm evaluation focused on metrics: Challenges and solutions},
  author={Hu, Taojun and Zhou, Xiao-Hua},
  journal={arXiv preprint arXiv:2404.09135},
  year={2024}
}

@ARTICLE{10855627,
  author={Zhao, Zhimin and Bangash, Abdul Ali and Côgo, Filipe Roseiro and Adams, Bram and Hassan, Ahmed E.},
  journal={IEEE Transactions on Software Engineering}, 
  title={On the Workflows and Smells of Leaderboard Operations (LBOps): An Exploratory Study of Foundation Model Leaderboards}, 
  year={2025},
  volume={51},
  number={4},
  pages={929-946},
  keywords={Frequency modulation;Biological system modeling;Reliability;Codes;Software development management;Software;Benchmark testing;Software reliability;Foundation models;Faces;Foundation model;machine learning leaderboard;mining software repositories;release engineering},
  doi={10.1109/TSE.2025.3533972}}


@article{orr2024building,
title={Building Better Datasets: Seven Recommendations for Responsible Design from Dataset Creators},
author={Will Orr and Kate Crawford},
journal={Journal of Data-centric Machine Learning Research},
issn={XXXX-XXXX},
year={2024},
url={https://openreview.net/forum?id=6bd8BrRKTW},
note={}
}

@article{singh2025leaderboard,
  title={The Leaderboard Illusion},
  author={Singh, Shivalika and Nan, Yiyang and Wang, Alex and D'Souza, Daniel and Kapoor, Sayash and {\"U}st{\"u}n, Ahmet and Koyejo, Sanmi and Deng, Yuntian and Longpre, Shayne and Smith, Noah and others},
  journal={arXiv preprint arXiv:2504.20879},
  year={2025}
}

@article{bommasani2023holistic,
  title={Holistic evaluation of language models},
  author={Bommasani, Rishi and Liang, Percy and Lee, Tony},
  journal={Annals of the New York Academy of Sciences},
  volume={1525},
  number={1},
  pages={140--146},
  year={2023},
  publisher={Wiley Online Library}
}

@article{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{liu-etal-2024-automatic,
    title = "Automatic Generation of Model and Data Cards: A Step Towards Responsible {AI}",
    author = "Liu, Jiarui  and
      Li, Wenkai  and
      Jin, Zhijing  and
      Diab, Mona",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.110/",
    doi = "10.18653/v1/2024.naacl-long.110",
    pages = "1975--1997",
    abstract = "In an era of model and data proliferation in machine learning/AI especially marked by the rapid advancement of open-sourced technologies, there arises a critical need for standardized consistent documentation. Our work addresses the information incompleteness in current human-written model and data cards. We propose an automated generation approach using Large Language Models (LLMs). Our key contributions include the establishment of CardBench, a comprehensive dataset aggregated from over 4.8k model cards and 1.4k data cards, coupled with the development of the CardGen pipeline comprising a two-step retrieval process. Our approach exhibits enhanced completeness, objectivity, and faithfulness in generated model and data cards, a significant step in responsible AI documentation practices ensuring better accountability and traceability."
}

@article{10.1145/3317287.3328534,
author = {Lipton, Zachary C. and Steinhardt, Jacob},
title = {Troubling Trends in Machine Learning Scholarship: Some ML papers suffer from flaws that could mislead the public and stymie future research.},
year = {2019},
issue_date = {January-February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1542-7730},
url = {https://doi.org/10.1145/3317287.3328534},
doi = {10.1145/3317287.3328534},
abstract = {Flawed scholarship threatens to mislead the public and stymie future research by compromising ML’s intellectual foundations. Indeed, many of these problems have recurred cyclically throughout the history of AI and, more broadly, in scientific research. In 1976, Drew McDermott chastised the AI community for abandoning self-discipline, warning prophetically that "if we can’t criticize ourselves, someone else will save us the trouble." The current strength of machine learning owes to a large body of rigorous research to date, both theoretical and empirical. By promoting clear scientific thinking and communication, our community can sustain the trust and investment it currently enjoys.},
journal = {Queue},
month = feb,
pages = {45–77},
numpages = {33}
}

@inproceedings{
sclar2024quantifying,
title={Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting},
author={Melanie Sclar and Yejin Choi and Yulia Tsvetkov and Alane Suhr},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=RIu5lyNXjT}
}

@article{allen2021evaluation,
  title={Evaluation and real-world performance monitoring of artificial intelligence models in clinical practice: try it, buy it, check it},
  author={Allen, Bibb and Dreyer, Keith and Stibolt Jr, Robert and Agarwal, Sheela and Coombs, Laura and Treml, Chris and Elkholy, Mona and Brink, Laura and Wald, Christoph},
  journal={Journal of the American College of Radiology},
  volume={18},
  number={11},
  pages={1489--1496},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{talat-etal-2022-reap,
    title = "You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings",
    author = "Talat, Zeerak  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Biderman, Stella  and
      Clinciu, Miruna  and
      Dey, Manan  and
      Longpre, Shayne  and
      Luccioni, Sasha  and
      Masoud, Maraim  and
      Mitchell, Margaret  and
      Radev, Dragomir  and
      Sharma, Shanya  and
      Subramonian, Arjun  and
      Tae, Jaesung  and
      Tan, Samson  and
      Tunuguntla, Deepak  and
      Van Der Wal, Oskar",
    editor = "Fan, Angela  and
      Ilic, Suzana  and
      Wolf, Thomas  and
      Gall{\'e}, Matthias",
    booktitle = "Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models",
    month = may,
    year = "2022",
    address = "virtual+Dublin",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.bigscience-1.3/",
    doi = "10.18653/v1/2022.bigscience-1.3",
    pages = "26--41",
    abstract = "Evaluating bias, fairness, and social impact in monolingual language models is a difficult task. This challenge is further compounded when language modeling occurs in a multilingual context. Considering the implication of evaluation biases for large multilingual language models, we situate the discussion of bias evaluation within a wider context of social scientific research with computational work. We highlight three dimensions of developing multilingual bias evaluation frameworks: (1) increasing transparency through documentation, (2) expanding targets of bias beyond gender, and (3) addressing cultural differences that exist between languages. We further discuss the power dynamics and consequences of training large language models and recommend that researchers remain cognizant of the ramifications of developing such technologies."
}

@inproceedings{joshi-etal-2020-state,
    title = "The State and Fate of Linguistic Diversity and Inclusion in the {NLP} World",
    author = "Joshi, Pratik  and
      Santy, Sebastin  and
      Budhiraja, Amar  and
      Bali, Kalika  and
      Choudhury, Monojit",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.560/",
    doi = "10.18653/v1/2020.acl-main.560",
    pages = "6282--6293",
    abstract = "Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the {\textquotedblleft}language agnostic{\textquotedblright} status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind."
}

@inproceedings{blasi-etal-2022-systematic,
    title = "Systematic Inequalities in Language Technology Performance across the World`s Languages",
    author = "Blasi, Damian  and
      Anastasopoulos, Antonios  and
      Neubig, Graham",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.376/",
    doi = "10.18653/v1/2022.acl-long.376",
    pages = "5486--5505",
    abstract = "Natural language processing (NLP) systems have become a central technology in communication, education, medicine, artificial intelligence, and many other domains of research and development. While the performance of NLP methods has grown enormously over the last decade, this progress has been restricted to a minuscule subset of the world`s $\approx$6,500 languages. We introduce a framework for estimating the global utility of language technologies as revealed in a comprehensive snapshot of recent publications in NLP. Our analyses involve the field at large, but also more in-depth studies on both user-facing technologies (machine translation, language understanding, question answering, text-to-speech synthesis) as well as foundational NLP tasks (dependency parsing, morphological inflection). In the process, we (1) quantify disparities in the current state of NLP research, (2) explore some of its associated societal and academic factors, and (3) produce tailored recommendations for evidence-based policy making aimed at promoting more global and equitable language technologies. Data and code to reproduce the findings discussed in this paper areavailable on GitHub (\url{https://github.com/neubig/globalutility})."
}


@inproceedings{hada-etal-2024-large,
    title = "Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?",
    author = "Hada, Rishav  and
      Gumma, Varun  and
      de Wynter, Adrian  and
      Diddee, Harshita  and
      Ahmed, Mohamed  and
      Choudhury, Monojit  and
      Bali, Kalika  and
      Sitaram, Sunayana",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2024",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-eacl.71/",
    pages = "1051--1070",
    abstract = "Large Language Models (LLMs) excel in various Natural Language Processing (NLP) tasks, yet their evaluation, particularly in languages beyond the top 20, remains inadequate due to existing benchmarks and metrics limitations. Employing LLMs as evaluators to rank or score other models' outputs emerges as a viable solution, addressing the constraints tied to human annotators and established benchmarks. In this study, we explore the potential of LLM-based evaluators in enhancing multilingual evaluation by calibrating them against 20K human judgments across three text-generation tasks, five metrics, and eight languages. Our analysis reveals a bias in LLM-based evaluators towards higher scores, underscoring the necessity of calibration with native speaker judgments, especially in low-resource and non-Latin script languages, to ensure accurate evaluation of LLM performance across diverse languages."
}

@article{felin2014closed,
  title={Closed or open innovation? Problem solving and the governance choice},
  author={Felin, Teppo and Zenger, Todd R},
  journal={Research policy},
  volume={43},
  number={5},
  pages={914--925},
  year={2014},
  publisher={Elsevier}
}

@inproceedings{10.1145/3582269.3615599,
author = {Kotek, Hadas and Dockum, Rikker and Sun, David},
title = {Gender bias and stereotypes in Large Language Models},
year = {2023},
isbn = {9798400701139},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582269.3615599},
doi = {10.1145/3582269.3615599},
abstract = {Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs’ behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women’s occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person’s gender; (b) these choices align with people’s perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95\% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably.},
booktitle = {Proceedings of The ACM Collective Intelligence Conference},
pages = {12–24},
numpages = {13},
keywords = {bias, ethics, explanations, gender, large language models, occupations, stereotypes},
location = {Delft, Netherlands},
series = {CI '23}
}

@inproceedings{ousidhoum-etal-2021-probing,
    title = "Probing Toxic Content in Large Pre-Trained Language Models",
    author = "Ousidhoum, Nedjma  and
      Zhao, Xinran  and
      Fang, Tianqing  and
      Song, Yangqiu  and
      Yeung, Dit-Yan",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.329/",
    doi = "10.18653/v1/2021.acl-long.329",
    pages = "4262--4274",
    abstract = "Large pre-trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems. We propose a method based on logistic regression classifiers to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates. The templates are prompted by a name of a social group followed by a cause-effect relation. We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities. We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large-scale study, then we explain how to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs."
}

@article{reuel2024betterbench,
  title={BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices},
  author={Reuel-Lamparth, Anka and Hardy, Amelia and Smith, Chandler and Lamparth, Max and Hardy, Malcolm and Kochenderfer, Mykel J},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={21763--21813},
  year={2024}
}

@article{burnell2023rethink,
  title={Rethink reporting of evaluation results in AI},
  author={Burnell, Ryan and Schellaert, Wout and Burden, John and Ullman, Tomer D and Martinez-Plumed, Fernando and Tenenbaum, Joshua B and Rutar, Danaja and Cheke, Lucy G and Sohl-Dickstein, Jascha and Mitchell, Melanie and others},
  journal={Science},
  volume={380},
  number={6641},
  pages={136--138},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@misc{act2024eu,
  title={The EU Artificial Intelligence Act},
  author={Act, EU Artificial Intelligence},
  year={2024},
  publisher={Retrieved May}
}

@ARTICLE{9380498,
  author={Zicari, Roberto V. and Brodersen, John and Brusseau, James and Düdder, Boris and Eichhorn, Timo and Ivanov, Todor and Kararigas, Georgios and Kringen, Pedro and McCullough, Melissa and Möslein, Florian and Mushtaq, Naveed and Roig, Gemma and Stürtz, Norman and Tolle, Karsten and Tithi, Jesmin Jahan and van Halem, Irmhild and Westerlund, Magnus},
  journal={IEEE Transactions on Technology and Society}, 
  title={Z-Inspection®: A Process to Assess Trustworthy AI}, 
  year={2021},
  volume={2},
  number={2},
  pages={83-97},
  keywords={Artificial intelligence;Inspection;Law;Organizations;Europe;Legal factors;Ethics;Accountability;artificial intelligence (AI);AI ethics;AI policy;AI-audit;algorithmic audits;corporate social responsibility;deep learning (DL);ethics;law;responsible innovation;society;machine learning (ML);Z-Inspection},
  doi={10.1109/TTS.2021.3066209}}


@article{ott2022mapping,
  title={Mapping global dynamics of benchmark creation and saturation in artificial intelligence},
  author={Ott, Simon and Barbosa-Silva, Adriano and Blagec, Kathrin and Brauner, Jan and Samwald, Matthias},
  journal={Nature Communications},
  volume={13},
  number={1},
  pages={6793},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{reuel2024open,
  title={Open problems in technical ai governance},
  author={Reuel, Anka and Bucknall, Ben and Casper, Stephen and Fist, Tim and Soder, Lisa and Aarne, Onni and Hammond, Lewis and Ibrahim, Lujain and Chan, Alan and Wills, Peter and others},
  journal={arXiv preprint arXiv:2407.14981},
  year={2024}
}

@inproceedings{reuel2024position,
  title={Position: Technical research and talent is needed for effective AI governance},
  author={Reuel, Anka and Soder, Lisa and Bucknall, Benjamin and Undheim, Trond Arne},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@misc{olmo20252olmo2furious,
      title={2 OLMo 2 Furious}, 
      author={Team OLMo and Pete Walsh and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Shane Arora and Akshita Bhagia and Yuling Gu and Shengyi Huang and Matt Jordan and Nathan Lambert and Dustin Schwenk and Oyvind Tafjord and Taira Anderson and David Atkinson and Faeze Brahman and Christopher Clark and Pradeep Dasigi and Nouha Dziri and Michal Guerquin and Hamish Ivison and Pang Wei Koh and Jiacheng Liu and Saumya Malik and William Merrill and Lester James V. Miranda and Jacob Morrison and Tyler Murray and Crystal Nam and Valentina Pyatkin and Aman Rangapur and Michael Schmitz and Sam Skjonsberg and David Wadden and Christopher Wilhelm and Michael Wilson and Luke Zettlemoyer and Ali Farhadi and Noah A. Smith and Hannaneh Hajishirzi},
      year={2025},
      eprint={2501.00656},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.00656}, 
}

@article{gemini15,
  publtype={informal},
  author={Machel Reid and Nikolay Savinov and Denis Teplyashin and Dmitry Lepikhin and Timothy P. Lillicrap and Jean-Baptiste Alayrac and Radu Soricut and Angeliki Lazaridou and Orhan Firat and Julian Schrittwieser and Ioannis Antonoglou and Rohan Anil and Sebastian Borgeaud and Andrew M. Dai and Katie Millican and Ethan Dyer and Mia Glaese and Thibault Sottiaux and Benjamin Lee and Fabio Viola and Malcolm Reynolds and Yuanzhong Xu and James Molloy and Jilin Chen and Michael Isard and Paul Barham and Tom Hennigan and Ross McIlroy and Melvin Johnson and Johan Schalkwyk and Eli Collins and Eliza Rutherford and Erica Moreira and Kareem Ayoub and Megha Goel and Clemens Meyer and Gregory Thornton and Zhen Yang and Henryk Michalewski and Zaheer Abbas and Nathan Schucher and Ankesh Anand and Richard Ives and James Keeling and Karel Lenc and Salem Haykal and Siamak Shakeri and Pranav Shyam and Aakanksha Chowdhery and Roman Ring and Stephen Spencer and Eren Sezener and et al.},
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2403.05530},
  url={https://doi.org/10.48550/arXiv.2403.05530}
}

@article{biderman2024lessons,
  title={Lessons from the trenches on reproducible evaluation of language models},
  author={Biderman, Stella and Schoelkopf, Hailey and Sutawika, Lintang and Gao, Leo and Tow, Jonathan and Abbasi, Baber and Aji, Alham Fikri and Ammanamanchi, Pawan Sasanka and Black, Sidney and Clive, Jordan and others},
  journal={arXiv preprint arXiv:2405.14782},
  year={2024}
}

@article{radanliev2024ethics,
  title={Ethics and responsible AI deployment},
  author={Radanliev, Petar and Santos, Omar and Brandon-Jones, Alistair and Joinson, Adam},
  journal={Frontiers in Artificial Intelligence},
  volume={7},
  pages={1377011},
  year={2024},
  publisher={Frontiers Media SA}
}

@article{tripathi2025ethical,
  title={Ethical practices of artificial intelligence: a management framework for responsible AI deployment in businesses},
  author={Tripathi, Ajay and Kumar, Vinod},
  journal={AI and Ethics},
  pages={1--12},
  year={2025},
  publisher={Springer}
}

@ARTICLE{10536000,
  author={Berengueres, J.},
  journal={IEEE Transactions on Technology and Society}, 
  title={How to Regulate Large Language Models for Responsible AI}, 
  year={2024},
  volume={5},
  number={2},
  pages={191-197},
  keywords={Ethics;Codes;Artificial intelligence;Benchmark testing;Regulation;General Data Protection Regulation;Large language models;Predictive models;Probabilistic logic;Training;Data integrity;Information integrity;Data integrity;Artificial intelligence;ethical computing;codes of ethics;algorithmic bias;AI governance;accountability in AI;responsible AI},
  doi={10.1109/TTS.2024.3403681}}

@inproceedings{mcgregor2025err,
  title={To err is ai: A case study informing llm flaw reporting practices},
  author={McGregor, Sean and Ettinger, Allyson and Judd, Nick and Albee, Paul and Jiang, Liwei and Rao, Kavel and Smith, William H and Longpre, Shayne and Ghosh, Avijit and Fiorelli, Christopher and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  pages={28938--28945},
  year={2025}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{hui2024qwen2,
  title={Qwen2. 5-coder technical report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Lu, Keming and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}

@article{toshniwal2024openmathinstruct,
  title={Openmathinstruct-1: A 1.8 million math instruction tuning dataset},
  author={Toshniwal, Shubham and Moshkov, Ivan and Narenthiran, Sean and Gitman, Daria and Jia, Fei and Gitman, Igor},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={34737--34774},
  year={2024}
}

@inproceedings{nie2024fine,
  title={Fine-Tuning Large Language Models for Solving Math Word Problems: A Case Study with LLaMA-3},
  author={Nie, Haoyu},
  booktitle={2024 4th International Signal Processing, Communications and Engineering Management Conference (ISPCEM)},
  pages={100--104},
  year={2024},
  organization={IEEE}
}

@inproceedings{crisan2022interactive,
  title={Interactive model cards: A human-centered approach to model documentation},
  author={Crisan, Anamaria and Drouhard, Margaret and Vig, Jesse and Rajani, Nazneen},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={427--439},
  year={2022}
}

@article{kopf2023openassistant,
  title={Openassistant conversations-democratizing large language model alignment},
  author={K{\"o}pf, Andreas and Kilcher, Yannic and Von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi Rui and Stevens, Keith and Barhoum, Abdullah and Nguyen, Duc and Stanley, Oliver and Nagyfi, Rich{\'a}rd and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={47669--47681},
  year={2023}
}

@article{team2025gemma,
  title={Gemma 3 technical report},
  author={Team, Gemma and Kamath, Aishwarya and Ferret, Johan and Pathak, Shreya and Vieillard, Nino and Merhej, Ramona and Perrin, Sarah and Matejovicova, Tatiana and Ram{\'e}, Alexandre and Rivi{\`e}re, Morgane and others},
  journal={arXiv preprint arXiv:2503.19786},
  year={2025}
}

@article{shekhar2024towards,
  title={Towards optimizing the costs of llm usage},
  author={Shekhar, Shivanshu and Dubey, Tanishq and Mukherjee, Koyel and Saxena, Apoorv and Tyagi, Atharv and Kotla, Nishanth},
  journal={arXiv preprint arXiv:2402.01742},
  year={2024}
}

@inproceedings{tedeschi-etal-2023-whats,
    title = "What`s the Meaning of Superhuman Performance in Today`s {NLU}?",
    author = "Tedeschi, Simone  and
      Bos, Johan  and
      Declerck, Thierry  and
      Haji{\v{c}}, Jan  and
      Hershcovich, Daniel  and
      Hovy, Eduard  and
      Koller, Alexander  and
      Krek, Simon  and
      Schockaert, Steven  and
      Sennrich, Rico  and
      Shutova, Ekaterina  and
      Navigli, Roberto",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.697/",
    doi = "10.18653/v1/2023.acl-long.697",
    pages = "12471--12491",
    abstract = "In the last five years, there has been a significant focus in Natural Language Processing (NLP) on developing larger Pretrained Language Models (PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their abilities in language understanding, reasoning, and reading comprehension. These PLMs have achieved impressive results on these benchmarks, even surpassing human performance in some cases. This has led to claims of superhuman capabilities and the provocative idea that certain tasks have been solved. In this position paper, we take a critical look at these claims and ask whether PLMs truly have superhuman abilities and what the current benchmarks are really evaluating. We show that these benchmarks have serious limitations affecting the comparison between humans and PLMs and provide recommendations for fairer and more transparent benchmarks."
}

@article{cao2025toward,
  title={Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks},
  author={Cao, Yixin and Hong, Shibo and Li, Xinze and Ying, Jiahao and Ma, Yubo and Liang, Haiyuan and Liu, Yantao and Yao, Zijun and Wang, Xiaozhi and Huang, Dan and others},
  journal={arXiv preprint arXiv:2504.18838},
  year={2025}
}

@inproceedings{perkovic2024hallucinations,
  title={Hallucinations in llms: Understanding and addressing challenges},
  author={Perkovi{\'c}, Gabrijela and Drobnjak, Antun and Boti{\v{c}}ki, Ivica},
  booktitle={2024 47th MIPRO ICT and Electronics Convention (MIPRO)},
  pages={2084--2088},
  year={2024},
  organization={IEEE}
}

@inproceedings{zhang2024toward,
  title={Toward mitigating misinformation and social media manipulation in llm era},
  author={Zhang, Yizhou and Sharma, Karishma and Du, Lun and Liu, Yan},
  booktitle={Companion Proceedings of the ACM Web Conference 2024},
  pages={1302--1305},
  year={2024}
}

@article{chen2024combating,
  title={Combating misinformation in the age of llms: Opportunities and challenges},
  author={Chen, Canyu and Shu, Kai},
  journal={AI Magazine},
  volume={45},
  number={3},
  pages={354--368},
  year={2024},
  publisher={Wiley Online Library}
}

@article{10.1145/3703155,
author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
title = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3703155},
doi = {10.1145/3703155},
abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {42},
numpages = {55},
keywords = {Large Language Models, Hallucination, Factuality, Faithfulness}
}


@inproceedings{luong-etal-2024-realistic,
    title = "Realistic Evaluation of Toxicity in Large Language Models",
    author = "Luong, Tinh  and
      Le, Thanh-Thien  and
      Ngo, Linh  and
      Nguyen, Thien",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.61/",
    doi = "10.18653/v1/2024.findings-acl.61",
    pages = "1038--1047",
    abstract = "Large language models (LLMs) have become integral to our professional workflows and daily lives. Nevertheless, these machine companions of ours have a critical flaw: the huge amount of data which endows them with vast and diverse knowledge, also exposes them to the inevitable toxicity and bias. While most LLMs incorporate defense mechanisms to prevent the generation of harmful content, these safeguards can be easily bypassed with minimal prompt engineering. In this paper, we introduce the new Thoroughly Engineered Toxicity (TET) dataset, comprising manually crafted prompts designed to nullify the protective layers of such models. Through extensive evaluations, we demonstrate the pivotal role of TET in providing a rigorous benchmark for evaluation of toxicity awareness in several popular LLMs: it highlights the toxicity in the LLMs that might remain hidden when using normal prompts, thus revealing subtler issues in their behavior."
}

@inproceedings{dai2024bias,
  title={Bias and unfairness in information retrieval systems: New challenges in the llm era},
  author={Dai, Sunhao and Xu, Chen and Xu, Shicheng and Pang, Liang and Dong, Zhenhua and Xu, Jun},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={6437--6447},
  year={2024}
}


@article{mahmood2024pricing,
  title={Pricing and competition for generative ai},
  author={Mahmood, Rafid},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={75727--75748},
  year={2024}
}

@article{fosso2024chatgpt,
  title={ChatGPT and generative artificial intelligence: an exploratory study of key benefits and challenges in operations and supply chain management},
  author={Fosso Wamba, Samuel and Guthrie, Cameron and Queiroz, Maciel M and Minner, Stefan},
  journal={International Journal of Production Research},
  volume={62},
  number={16},
  pages={5676--5696},
  year={2024},
  publisher={Taylor \& Francis}
}

@article{doo2024economic,
  title={Economic and environmental costs of cloud technologies for medical imaging and radiology artificial intelligence},
  author={Doo, Florence X and Kulkarni, Pranav and Siegel, Eliot L and Toland, Michael and Yi, Paul H and Carlos, Ruth C and Parekh, Vishwa S},
  journal={Journal of the American College of Radiology},
  volume={21},
  number={2},
  pages={248--256},
  year={2024},
  publisher={Elsevier}
}

@article{ye2023assessing,
  title={Assessing hidden risks of LLMs: an empirical study on robustness, consistency, and credibility},
  author={Ye, Wentao and Ou, Mingfeng and Li, Tianyi and Ma, Xuetao and Yanggong, Yifan and Wu, Sai and Fu, Jie and Chen, Gang and Wang, Haobo and Zhao, Junbo and others},
  journal={arXiv preprint arXiv:2305.10235},
  year={2023}
}

@inproceedings{hu2024use,
  title={The use and misuse of pre-trained generative large language models in reliability engineering},
  author={Hu, Yunwei and Goktas, Yavuz and Yellamati, David Deepak and De Tassigny, Catherine},
  booktitle={2024 Annual Reliability and Maintainability Symposium (RAMS)},
  pages={1--7},
  year={2024},
  organization={IEEE}
}

@article{li2023chatdoctor,
  title={Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge},
  author={Li, Yunxiang and Li, Zihan and Zhang, Kai and Dan, Ruilong and Jiang, Steve and Zhang, You},
  journal={Cureus},
  volume={15},
  number={6},
  year={2023},
  publisher={Cureus}
}

@article{shah2018algorithmic,
  title={Algorithmic accountability},
  author={Shah, Hetan},
  journal={Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume={376},
  number={2128},
  pages={20170362},
  year={2018},
  publisher={The Royal Society Publishing}
}

@article{hogan2021ethics,
  title={On the Ethics and Practicalities of Artificial Intelligence, Risk Assessment, and Race.},
  author={Hogan, Neil R and Davidge, Ethan Q and Corabian, Gabriela},
  journal={The Journal of the American Academy of Psychiatry and the Law},
  volume={49},
  number={3},
  pages={326--334},
  year={2021}
}

@article{novelli2024taking,
  title={Taking AI risks seriously: a new assessment model for the AI Act},
  author={Novelli, Claudio and Casolari, Federico and Rotolo, Antonino and Taddeo, Mariarosaria and Floridi, Luciano},
  journal={AI \& SOCIETY},
  volume={39},
  number={5},
  pages={2493--2497},
  year={2024},
  publisher={Springer}
}



@inproceedings{wieringa2020account,
  title={What to account for when accounting for algorithms: a systematic literature review on algorithmic accountability},
  author={Wieringa, Maranke},
  booktitle={Proceedings of the 2020 conference on fairness, accountability, and transparency},
  pages={1--18},
  year={2020}
}

@article{horneber2023algorithmic,
  title={Algorithmic accountability},
  author={Horneber, David and Laumer, Sven},
  journal={Business \& information systems engineering},
  volume={65},
  number={6},
  pages={723--730},
  year={2023},
  publisher={Springer}
}

@article{koshiyama2024towards,
  title={Towards algorithm auditing: managing legal, ethical and technological risks of AI, ML and associated algorithms},
  author={Koshiyama, Adriano and Kazim, Emre and Treleaven, Philip and Rai, Pete and Szpruch, Lukasz and Pavey, Giles and Ahamat, Ghazi and Leutner, Franziska and Goebel, Randy and Knight, Andrew and others},
  journal={Royal Society Open Science},
  volume={11},
  number={5},
  pages={230859},
  year={2024},
  publisher={The Royal Society}
}

@inproceedings{huijgens2024help,
  title={Help! which ai should i choose?-work in progress},
  author={Huijgens, Hennie and Eskiyerli, Devrim and K{\"o}r, Burcu},
  booktitle={The 1st Human-Centered AI Education Practice Conference, Dublin, Ireland},
  year={2024}
}

@article{mesko2023imperative,
  title={The imperative for regulatory oversight of large language models (or generative AI) in healthcare},
  author={Mesk{\'o}, Bertalan and Topol, Eric J},
  journal={NPJ digital medicine},
  volume={6},
  number={1},
  pages={120},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{liu-etal-2024-ecbd,
    title = "{ECBD}: Evidence-Centered Benchmark Design for {NLP}",
    author = "Liu, Yu Lu  and
      Blodgett, Su Lin  and
      Cheung, Jackie  and
      Liao, Q. Vera  and
      Olteanu, Alexandra  and
      Xiao, Ziang",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.861/",
    doi = "10.18653/v1/2024.acl-long.861",
    pages = "16349--16365",
    abstract = "Benchmarking is seen as critical to assessing progress in NLP. However, creating a benchmark involves many design decisions (e.g., which datasets to include, which metrics to use) that often rely on tacit, untested assumptions about what the benchmark is intended to measure or is actually measuring. There is currently no principled way of analyzing these decisions and how they impact the validity of the benchmark`s measurements. To address this gap, we draw on evidence-centered design in educational assessments and propose Evidence-Centered Benchmark Design (ECBD), a framework which formalizes the benchmark design process into five modules. ECBD specifies the role each module plays in helping practitioners collect evidence about capabilities of interest. Specifically, each module requires benchmark designers to describe, justify, and support benchmark design choices{---}e.g., clearly specifying the capabilities the benchmark aims to measure or how evidence about those capabilities is collected from model responses. To demonstrate the use of ECBD, we conduct case studies with three benchmarks: BoolQ, SuperGLUE, and HELM. Our analysis reveals common trends in benchmark design and documentation that could threaten the validity of benchmarks' measurements."
}

@article{veale2021demystifying,
  title={Demystifying the Draft EU Artificial Intelligence Act—Analysing the good, the bad, and the unclear elements of the proposed approach},
  author={Veale, Michael and Zuiderveen Borgesius, Frederik},
  journal={Computer Law Review International},
  volume={22},
  number={4},
  pages={97--112},
  year={2021},
  publisher={Verlag Dr. Otto Schmidt}
}

@article{cohen1988evaluation,
  title={How evaluation guides AI research: The message still counts more than the medium},
  author={Cohen, Paul R and Howe, Adele E},
  journal={AI magazine},
  volume={9},
  number={4},
  pages={35--35},
  year={1988}
}

@article{hernandez2017evaluation,
  title={Evaluation in artificial intelligence: from task-oriented to ability-oriented measurement},
  author={Hern{\'a}ndez-Orallo, Jos{\'e}},
  journal={Artificial Intelligence Review},
  volume={48},
  pages={397--447},
  year={2017},
  publisher={Springer}
}

@inproceedings{raji2020closing,
  title={Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing},
  author={Raji, Inioluwa Deborah and Smart, Andrew and White, Rebecca N and Mitchell, Margaret and Gebru, Timnit and Hutchinson, Ben and Smith-Loud, Jamila and Theron, Daniel and Barnes, Parker},
  booktitle={Proceedings of the 2020 conference on fairness, accountability, and transparency},
  pages={33--44},
  year={2020}
}

@article{bowman2021will,
  title={What will it take to fix benchmarking in natural language understanding?},
  author={Bowman, Samuel R and Dahl, George E},
  journal={arXiv preprint arXiv:2104.02145},
  year={2021}
}

@inproceedings{10.5555/3042573.3042809,
author = {Wagstaff, Kiri L.},
title = {Machine learning that matters},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field's energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1851–1856},
numpages = {6},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@article{smuha2021race,
  title={From a ‘race to AI’to a ‘race to AI regulation’: regulatory competition for artificial intelligence},
  author={Smuha, Nathalie A},
  journal={Law, Innovation and Technology},
  volume={13},
  number={1},
  pages={57--84},
  year={2021},
  publisher={Taylor \& Francis}
}

@incollection{ulnicane2022artificial,
  title={Artificial Intelligence in the European Union: Policy, ethics and regulation},
  author={Ulnicane, Inga},
  booktitle={The Routledge handbook of European integrations},
  year={2022},
  publisher={Taylor \& Francis}
}

@article{luccioni2023estimating,
  title={Estimating the carbon footprint of bloom, a 176b parameter language model},
  author={Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={253},
  pages={1--15},
  year={2023}
}

@article{luccioni2024environmental,
  title={The environmental impacts of ai--primer},
  author={Luccioni, Sasha and Trevelin, Bruna and Mitchell, Margaret},
  journal={Hugging Face Blog},
  year={2024}
}

@article{luccioni2025bridging,
  title={Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice},
  author={Luccioni, Alexandra Sasha and Pistilli, Giada and Sefala, Raesetje and Moorosi, Nyalleng},
  journal={arXiv preprint arXiv:2504.00797},
  year={2025}
}

@book{roberts2021chinese,
  title={The Chinese approach to artificial intelligence: an analysis of policy, ethics, and regulation},
  author={Roberts, Huw and Cowls, Josh and Morley, Jessica and Taddeo, Mariarosaria and Wang, Vincent and Floridi, Luciano},
  year={2021},
  publisher={Springer}
}

@article{shetty2025analyzing,
  title={Analyzing AI Regulation through Literature and Current Trends},
  author={Shetty, Dasharathraj K and Arjunan, R Vijaya and Cenitta, D and Makkithaya, Krishnamoorthi and Hegde, Nikhil Venkatraman and Salu, Staissy and Aishwarya, TR and Bhat, Pranav and Pullela, Phani Kumar and others},
  journal={Journal of Open Innovation: Technology, Market, and Complexity},
  pages={100508},
  year={2025},
  publisher={Elsevier}
}

@inproceedings{bhardwaj2024machine,
  title={Machine learning data practices through a data curation lens: An evaluation framework},
  author={Bhardwaj, Eshta and Gujral, Harshit and Wu, Siyi and Zogheib, Ciara and Maharaj, Tegan and Becker, Christoph},
  booktitle={Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1055--1067},
  year={2024}
}

@article{arnold2019factsheets,
  title={FactSheets: Increasing trust in AI services through supplier's declarations of conformity},
  author={Arnold, Matthew and Bellamy, Rachel KE and Hind, Michael and Houde, Stephanie and Mehta, Sameep and Mojsilovi{\'c}, Aleksandra and Nair, Ravi and Ramamurthy, K Natesan and Olteanu, Alexandra and Piorkowski, David and others},
  journal={IBM Journal of Research and Development},
  volume={63},
  number={4/5},
  pages={6--1},
  year={2019},
  publisher={IBM}
}

@inproceedings{akyurek-etal-2022-challenges,
    title = "Challenges in Measuring Bias via Open-Ended Language Generation",
    author = {Aky{\"u}rek, Afra Feyza  and
      Kocyigit, Muhammed Yusuf  and
      Paik, Sejin  and
      Wijaya, Derry Tanti},
    editor = "Hardmeier, Christian  and
      Basta, Christine  and
      Costa-juss{\`a}, Marta R.  and
      Stanovsky, Gabriel  and
      Gonen, Hila",
    booktitle = "Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP)",
    month = jul,
    year = "2022",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.gebnlp-1.9/",
    doi = "10.18653/v1/2022.gebnlp-1.9",
    pages = "76--76",
    abstract = "Researchers have devised numerous ways to quantify social biases vested in pretrained language models. As some language models are capable of generating coherent completions given a set of textual prompts, several prompting datasets have been proposed to measure biases between social groups{---}posing language generation as a way of identifying biases. In this opinion paper, we analyze how specific choices of prompt sets, metrics, automatic tools and sampling strategies affect bias results. We find out that the practice of measuring biases through text completion is prone to yielding contradicting results under different experiment settings. We additionally provide recommendations for reporting biases in open-ended language generation for a more complete outlook of biases exhibited by a given language model. Code to reproduce the results is released under \url{https://github.com/feyzaakyurek/bias-textgen}."
}

@inproceedings{blodgett-etal-2021-stereotyping,
    title = "Stereotyping {N}orwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
    author = "Blodgett, Su Lin  and
      Lopez, Gilsinia  and
      Olteanu, Alexandra  and
      Sim, Robert  and
      Wallach, Hanna",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.81/",
    doi = "10.18653/v1/2021.acl-long.81",
    pages = "1004--1015",
    abstract = "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system`s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens{---}originating from the social sciences{---}to inventory a range of pitfalls that threaten these benchmarks' validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping."
}

@inproceedings{blodgett-etal-2024-human,
    title = "Human-Centered Evaluation of Language Technologies",
    author = "Blodgett, Su Lin  and
      Cheung, Jackie Chi Kit  and
      Liao, Vera  and
      Xiao, Ziang",
    editor = "Li, Jessy  and
      Liu, Fei",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-tutorials.6/",
    doi = "10.18653/v1/2024.emnlp-tutorials.6",
    pages = "39--43",
    abstract = "Evaluation is a cornerstone topic in NLP. However, many criticisms have been raised about the community`s evaluation practices, including a lack of human-centered considerations about people`s needs for language technologies and their actual impact on people. This {\textquotedblleft}evaluation crisis{\textquotedblright} is exacerbated by the recent development of large generative models with diverse and uncertain capabilities. This tutorial aims to inspire more human-centered evaluation in NLP by introducing perspectives and methodologies from human-computer interaction (HCI), a field concerned primarily with the design and evaluation of technologies. The tutorial will start with an overview of current NLP evaluation practices and their limitations, then introduce the {\textquotedblleft}toolbox of evaluation methods{\textquotedblright} from HCI with varying considerations such as what to evaluate for, how generalizable the results are to the real-world contexts, and pragmatic costs to conduct the evaluation. The tutorial will also encourage reflection on how these HCI perspectives and methodologies can complement NLP evaluation through Q{\&}A discussions and a hands-on exercise."
}

@article{gebru2021datasheets,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daum{\'e} and Crawford, Kate},
  journal={Communications of the ACM},
  volume={64},
  number={12},
  pages={86--92},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{novelli2024accountability,
  title={Accountability in artificial intelligence: what it is and how it works},
  author={Novelli, Claudio and Taddeo, Mariarosaria and Floridi, Luciano},
  journal={Ai \& Society},
  volume={39},
  number={4},
  pages={1871--1882},
  year={2024},
  publisher={Springer}
}

@inproceedings{xia2024towards,
  title={Towards a responsible ai metrics catalogue: A collection of metrics for ai accountability},
  author={Xia, Boming and Lu, Qinghua and Zhu, Liming and Lee, Sung Une and Liu, Yue and Xing, Zhenchang},
  booktitle={Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering-Software Engineering for AI},
  pages={100--111},
  year={2024}
}

@inproceedings{mitchell2019model,
  title={Model cards for model reporting},
  author={Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={220--229},
  year={2019}
}


@misc{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chadrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  year={2023},
  publisher={ArXiv}
}

@misc{anthropicclaude,
	author = {Anthropic},
	title = {{T}he {C}laude 3 {M}odel {F}amily: {O}pus, {S}onnet, {H}aiku},
	howpublished = {\url{https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf}},
	year = {},
	note = {[Accessed 16-04-2025]},
}

@article{kudina2024sociotechnical,
  title={A sociotechnical system perspective on AI},
  author={Kudina, Olya and van de Poel, Ibo},
  journal={Minds and Machines},
  volume={34},
  number={3},
  pages={21},
  year={2024},
  publisher={Springer}
}